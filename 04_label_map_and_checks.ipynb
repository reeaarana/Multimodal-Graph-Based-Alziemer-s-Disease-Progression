{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbc04c2",
        "outputId": "3ee6fc7a-e0d6-4816-862a-6ccaba3acd37"
      },
      "source": [
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/oasis_project/data/demographics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf6ODJkuNk0u",
        "outputId": "4a0d964b-1a26-4f61-938e-075b05b98fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 52K\n",
            "-rw-r--r-- 1 root root 50K Oct  9 04:46 oasis2_demographics.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Unmount drive if already mounted and clear the mountpoint\n",
        "if os.path.exists('/content/drive'):\n",
        "    try:\n",
        "        drive.flush_and_unmount()\n",
        "        # It might take a moment for the unmount to complete,\n",
        "        # but often clearing the directory is sufficient.\n",
        "    except ValueError:\n",
        "        # Drive was not mounted, no need to unmount\n",
        "        pass\n",
        "\n",
        "    # Clear the mountpoint directory\n",
        "    if os.path.exists('/content/drive') and os.path.isdir('/content/drive'):\n",
        "        for item in os.listdir('/content/drive'):\n",
        "            item_path = os.path.join('/content/drive', item)\n",
        "            try:\n",
        "                if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                    os.unlink(item_path)\n",
        "                elif os.path.isdir(item_path):\n",
        "                    shutil.rmtree(item_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing {item_path}: {e}\")\n",
        "\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "print(\"‚úÖ Drive mounted.\")\n",
        "print(\"Drive contents at root:\")\n",
        "for f in os.listdir(\"/content/drive\"):\n",
        "    print(\" -\", f)\n",
        "\n",
        "# verify main project folder exists\n",
        "proj = \"/content/drive/MyDrive/oasis_project\"\n",
        "print(\"\\nProject folder exists?\", os.path.exists(proj))\n",
        "if os.path.exists(proj):\n",
        "    print(\"Contents of oasis_project:\")\n",
        "    for f in os.listdir(proj):\n",
        "        print(\"   \", f)"
      ],
      "metadata": {
        "id": "MyeZMAtORIfK",
        "outputId": "b277cfd4-dfcb-4de2-ca22-503d0e1cd65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n",
            "‚úÖ Drive mounted.\n",
            "Drive contents at root:\n",
            " - MyDrive\n",
            " - .shortcut-targets-by-id\n",
            " - .Trash-0\n",
            " - .Encrypted\n",
            "\n",
            "Project folder exists? True\n",
            "Contents of oasis_project:\n",
            "    notebooks\n",
            "    data\n",
            "    outputs\n",
            "    logs\n",
            "    oasis2_graph_dataset.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"üì§ Please upload your demographics Excel file (e.g., oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx)\")\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "6iybUDGQNXVk",
        "outputId": "386c65f2-0b87-4551-f13e-5f7571705649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Please upload your demographics Excel file (e.g., oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b2cf222-443c-4838-835f-6c3651d65b76\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2b2cf222-443c-4838-835f-6c3651d65b76\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx to oasis_longitudinal_demographics-8d83e569fa2e2d30.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "DEM_DIR = os.path.join(BASE, \"data\", \"demographics\")\n",
        "os.makedirs(DEM_DIR, exist_ok=True)\n",
        "\n",
        "# Detect uploaded file automatically\n",
        "uploaded_name = list(uploaded.keys())[0]\n",
        "src = f\"/content/{uploaded_name}\"\n",
        "dst = os.path.join(DEM_DIR, \"oasis2_demographics.xlsx\")\n",
        "\n",
        "shutil.move(src, dst)\n",
        "print(f\"‚úÖ Moved demographics file to: {dst}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnjYW_j5Nfdx",
        "outputId": "173c4535-0ca7-4659-ae2a-d5cea56eadb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Moved demographics file to: /content/drive/MyDrive/oasis_project/data/demographics/oasis2_demographics.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINALIZE LABELS: try safe fallback mapping (0003 -> OAS2_0003) and export final .pt\n",
        "import os, glob, re, numpy as np, pandas as pd, torch, shutil\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "NPZ_DIR = os.path.join(BASE, \"data\", \"graphs\", \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "AUTO_INDEX_CSV = os.path.join(BASE, \"data\", \"graphs\", \"graph_label_index_auto.csv\")\n",
        "AUTO_MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_auto.csv\")\n",
        "FINAL_INDEX_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_graph_label_index_auto.csv\")\n",
        "FINAL_MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
        "OUT_PT = os.path.join(BASE, \"data\", \"graphs\", \"oasis2_graphs_labeled_auto_final.pt\")\n",
        "\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "# --- load demographics keys ---\n",
        "if DEM_PATH.lower().endswith('.csv'):\n",
        "    df_dem = pd.read_csv(DEM_PATH)\n",
        "else:\n",
        "    df_dem = pd.read_excel(DEM_PATH)\n",
        "df_dem.rename(columns={c:c.strip() for c in df_dem.columns}, inplace=True)\n",
        "cols = list(df_dem.columns)\n",
        "id_col = next((c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), cols[0])\n",
        "label_col = next((c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), None)\n",
        "if label_col is None:\n",
        "    numeric_cols = [c for c in cols if np.issubdtype(df_dem[c].dtype, np.number)]\n",
        "    label_col = numeric_cols[0] if numeric_cols else cols[-1]\n",
        "dem_map = { str(x).strip(): df_dem.loc[i, label_col] for i,x in df_dem[id_col].items() if pd.notna(x) }\n",
        "dem_keys = set(dem_map.keys())\n",
        "\n",
        "print(\"Loaded dem keys sample:\", list(dem_keys)[:8], \" total:\", len(dem_keys))\n",
        "\n",
        "# --- load auto index + misses produced earlier ---\n",
        "auto_idx = pd.read_csv(AUTO_INDEX_CSV) if os.path.exists(AUTO_INDEX_CSV) else pd.DataFrame()\n",
        "misses = pd.read_csv(AUTO_MISSES_CSV) if os.path.exists(AUTO_MISSES_CSV) else pd.DataFrame()\n",
        "\n",
        "print(\"Auto-index rows:\", len(auto_idx), \"Auto-misses rows:\", len(misses))\n",
        "\n",
        "# --- helper: extract sid from npz filename used earlier ---\n",
        "def sid_from_fname(fname):\n",
        "    bn = os.path.basename(fname)\n",
        "    m = re.search(r'(subj[_-]\\d+|sub[_-]\\d+|subj\\d+|sub\\d+|\\d{2,})', bn, flags=re.IGNORECASE)\n",
        "    return (m.group(0) if m else os.path.splitext(bn)[0])\n",
        "\n",
        "def normalize(x):\n",
        "    if pd.isna(x): return None\n",
        "    return str(x).strip()\n",
        "\n",
        "# --- Attempt aggressive fallback mapping for misses: numeric -> OAS2_ + zero-pad(4) (only if key exists) ---\n",
        "added_rows = []\n",
        "remaining_misses = []\n",
        "for _, row in misses.iterrows():\n",
        "    p = row.get('npz_file') if 'npz_file' in row else row.get('file', None)\n",
        "    sid = row.get('npz_sid') if 'npz_sid' in row else sid_from_fname(p)\n",
        "    sid = normalize(sid)\n",
        "    mapped = None\n",
        "    if sid and sid.isdigit():\n",
        "        cand = \"OAS2_\" + sid.zfill(4)\n",
        "        if cand in dem_keys:\n",
        "            mapped = cand\n",
        "    # also try numeric-only without prefix\n",
        "    if mapped is None and sid and sid.isdigit() and sid in dem_keys:\n",
        "        mapped = sid\n",
        "    # also try \"OAS2-\" variant\n",
        "    if mapped is None and sid and sid.isdigit():\n",
        "        cand2 = \"OAS2-\" + sid.zfill(4)\n",
        "        if cand2 in dem_keys:\n",
        "            mapped = cand2\n",
        "\n",
        "    if mapped is not None:\n",
        "        # label and write labeled npz\n",
        "        arr = np.load(p, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([]))\n",
        "        pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "        y = np.array([dem_map[mapped]])\n",
        "        outname = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "        np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "        added_rows.append({\"npz_file\": p, \"labeled_npz\": outname, \"npz_sid\": sid, \"dem_key\": mapped, \"y\": float(dem_map[mapped])})\n",
        "    else:\n",
        "        remaining_misses.append({\"npz_file\": p, \"npz_sid\": sid, \"suggestions\": row.get('suggestions') if 'suggestions' in row else []})\n",
        "\n",
        "print(\"Aggressive fallback labeled:\", len(added_rows), \"Remaining misses after fallback:\", len(remaining_misses))\n",
        "\n",
        "# --- combine index and save final CSVs ---\n",
        "final_index = pd.concat([\n",
        "    auto_idx if not auto_idx.empty else pd.DataFrame(columns=[\"npz_file\",\"labeled_npz\",\"npz_sid\",\"dem_key\",\"y\"]),\n",
        "    pd.DataFrame(added_rows)\n",
        "], ignore_index=True)\n",
        "\n",
        "final_misses_df = pd.DataFrame(remaining_misses)\n",
        "\n",
        "final_index.to_csv(FINAL_INDEX_CSV, index=False)\n",
        "final_misses_df.to_csv(FINAL_MISSES_CSV, index=False)\n",
        "print(\"Saved final index:\", FINAL_INDEX_CSV)\n",
        "print(\"Saved final misses:\", FINAL_MISSES_CSV)\n",
        "print(\"Total labeled (auto + fallback):\", len(final_index))\n",
        "print(\"Remaining misses:\", len(final_misses_df))\n",
        "\n",
        "# --- Optional: build final .pt from *all* labeled NPZs (both originally auto-labeled and newly labeled) ---\n",
        "# We'll gather labeled files from final_index['npz_file'] to ensure consistent set\n",
        "labeled_files = final_index['npz_file'].tolist()\n",
        "data_list = []\n",
        "for p in tqdm(labeled_files, desc=\"Creating Data objects\"):\n",
        "    try:\n",
        "        arr = np.load(p, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([]))\n",
        "        pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "        y = final_index.loc[final_index['npz_file']==p, 'y'].values\n",
        "        # convert to tensors when possible\n",
        "        try:\n",
        "            xt = torch.tensor(x, dtype=torch.float) if getattr(x, \"size\", 0) else None\n",
        "            pt = torch.tensor(pos, dtype=torch.float) if getattr(pos, \"size\", 0) else None\n",
        "            eit = torch.tensor(ei, dtype=torch.long) if getattr(ei, \"size\", 0) else None\n",
        "            yt = torch.tensor(y, dtype=torch.float) if len(y)>0 else None\n",
        "            g = Data(x=xt, edge_index=eit, pos=pt, y=yt)\n",
        "            # set subject id if available\n",
        "            sid = sid_from_fname(p)\n",
        "            g.subject_id = sid\n",
        "            data_list.append(g)\n",
        "        except Exception as e:\n",
        "            # skip conversion errors, but continue\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(\"Failed loading npz\", p, e)\n",
        "\n",
        "if data_list:\n",
        "    torch.save(data_list, OUT_PT)\n",
        "    print(\"Saved final labeled .pt with\", len(data_list), \"graphs to:\", OUT_PT)\n",
        "else:\n",
        "    print(\"No Data objects created (nothing saved as .pt).\")\n",
        "\n",
        "# --- If misses remain, give instructions and small template to edit manually ---\n",
        "if len(final_misses_df) > 0:\n",
        "    print(\"\\nThere are remaining misses. Edit the CSV at:\\n  \", FINAL_MISSES_CSV)\n",
        "    print(\"Add a column named 'mapped_dem_key' and put the dem key (e.g. OAS2_0123) for each npz_file you want to map.\")\n",
        "    print(\"When done, run the small apply-manual-mapping block below (I included it here).\")\n",
        "\n",
        "    # show small apply-manual-mapping snippet for convenience\n",
        "    print(\"\\n--- To apply manual mappings (run after you edit), execute this block: ---\\n\")\n",
        "    print(r\"\"\"\n",
        "# APPLY manual mappings from final_label_misses_auto.csv (after editing)\n",
        "import pandas as pd, numpy as np, os\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
        "FINAL_MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "dfm = pd.read_csv(FINAL_MISSES_CSV)\n",
        "# expected columns: npz_file, npz_sid, suggestions, mapped_dem_key\n",
        "for _, r in dfm.iterrows():\n",
        "    p = r['npz_file']\n",
        "    mkey = r.get('mapped_dem_key')\n",
        "    if pd.isna(mkey) or not mkey: continue\n",
        "    # load dem and write labeled npz\n",
        "    if DEM_PATH.lower().endswith('.csv'):\n",
        "        dem = pd.read_csv(DEM_PATH)\n",
        "    else:\n",
        "        dem = pd.read_excel(DEM_PATH)\n",
        "    dem.rename(columns={c:c.strip() for c in dem.columns}, inplace=True)\n",
        "    idcol = next((c for c in dem.columns if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), dem.columns[0])\n",
        "    labcol = next((c for c in dem.columns if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), dem.columns[-1])\n",
        "    val = dem.loc[dem[idcol].astype(str).str.strip()==str(mkey).strip(), labcol]\n",
        "    if val.size==0:\n",
        "        print(\"mapped key not found in demographics:\", mkey); continue\n",
        "    y = np.array([float(val.iloc[0])])\n",
        "    arr = np.load(p, allow_pickle=True)\n",
        "    x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([])); ei = arr.get('edge_index', np.empty((2,0),dtype=np.int64))\n",
        "    outname = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "    np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "    print(\"Saved manual-labeled:\", outname)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nFinished finalization step.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVH95UukM7wt",
        "outputId": "661cbd2e-0022-44d0-f313-c5ee42ce3217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dem keys sample: ['OAS2_0081', 'OAS2_0035', 'OAS2_0114', 'OAS2_0112', 'OAS2_0175', 'OAS2_0181', 'OAS2_0046', 'OAS2_0022']  total: 150\n",
            "Auto-index rows: 150 Auto-misses rows: 59\n",
            "Aggressive fallback labeled: 0 Remaining misses after fallback: 59\n",
            "Saved final index: /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv\n",
            "Saved final misses: /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n",
            "Total labeled (auto + fallback): 150\n",
            "Remaining misses: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating Data objects: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:01<00:00, 82.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved final labeled .pt with 150 graphs to: /content/drive/MyDrive/oasis_project/data/graphs/oasis2_graphs_labeled_auto_final.pt\n",
            "\n",
            "There are remaining misses. Edit the CSV at:\n",
            "   /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n",
            "Add a column named 'mapped_dem_key' and put the dem key (e.g. OAS2_0123) for each npz_file you want to map.\n",
            "When done, run the small apply-manual-mapping block below (I included it here).\n",
            "\n",
            "--- To apply manual mappings (run after you edit), execute this block: ---\n",
            "\n",
            "\n",
            "# APPLY manual mappings from final_label_misses_auto.csv (after editing)\n",
            "import pandas as pd, numpy as np, os\n",
            "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
            "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
            "FINAL_MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
            "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
            "dfm = pd.read_csv(FINAL_MISSES_CSV)\n",
            "# expected columns: npz_file, npz_sid, suggestions, mapped_dem_key\n",
            "for _, r in dfm.iterrows():\n",
            "    p = r['npz_file']\n",
            "    mkey = r.get('mapped_dem_key')\n",
            "    if pd.isna(mkey) or not mkey: continue\n",
            "    # load dem and write labeled npz\n",
            "    if DEM_PATH.lower().endswith('.csv'):\n",
            "        dem = pd.read_csv(DEM_PATH)\n",
            "    else:\n",
            "        dem = pd.read_excel(DEM_PATH)\n",
            "    dem.rename(columns={c:c.strip() for c in dem.columns}, inplace=True)\n",
            "    idcol = next((c for c in dem.columns if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), dem.columns[0])\n",
            "    labcol = next((c for c in dem.columns if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), dem.columns[-1])\n",
            "    val = dem.loc[dem[idcol].astype(str).str.strip()==str(mkey).strip(), labcol]\n",
            "    if val.size==0: \n",
            "        print(\"mapped key not found in demographics:\", mkey); continue\n",
            "    y = np.array([float(val.iloc[0])])\n",
            "    arr = np.load(p, allow_pickle=True)\n",
            "    x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([])); ei = arr.get('edge_index', np.empty((2,0),dtype=np.int64))\n",
            "    outname = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
            "    np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
            "    print(\"Saved manual-labeled:\", outname)\n",
            "\n",
            "\n",
            "Finished finalization step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FUZZY RELABEL: produce suggestions for remaining misses and optionally apply high-confidence matches\n",
        "import os, glob, re, numpy as np, pandas as pd, difflib\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "FINAL_MISSES = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
        "FINAL_INDEX = os.path.join(BASE, \"data\", \"graphs\", \"final_graph_label_index_auto.csv\")\n",
        "NPZ_DIR = os.path.join(BASE, \"data\", \"graphs\", \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
        "OUT_SUGGESTIONS = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_fuzzy_suggestions.csv\")\n",
        "OUT_UPDATED_INDEX = os.path.join(BASE, \"data\", \"graphs\", \"final_graph_label_index_auto_postfuzzy.csv\")\n",
        "OUT_UPDATED_MISSES = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto_postfuzzy.csv\")\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "# Config\n",
        "TOP_K = 5                 # how many fuzzy candidates to keep per miss\n",
        "AUTO_APPLY = False        # set True to automatically label high-confidence matches\n",
        "AUTO_THRESHOLD = 0.85     # similarity threshold (0..1) for auto-apply\n",
        "PRINT_SAMPLE = True\n",
        "\n",
        "# --- load demographics keys & label values ---\n",
        "if DEM_PATH.lower().endswith('.csv'):\n",
        "    dem_df = pd.read_csv(DEM_PATH)\n",
        "else:\n",
        "    dem_df = pd.read_excel(DEM_PATH)\n",
        "dem_df.rename(columns={c:c.strip() for c in dem_df.columns}, inplace=True)\n",
        "cols = list(dem_df.columns)\n",
        "id_col = next((c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), cols[0])\n",
        "label_col = next((c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), None)\n",
        "if label_col is None:\n",
        "    numeric_cols = [c for c in cols if np.issubdtype(dem_df[c].dtype, np.number)]\n",
        "    label_col = numeric_cols[0] if numeric_cols else cols[-1]\n",
        "# build canonical key list (strings)\n",
        "dem_keys = [str(x).strip() for x in dem_df[id_col].astype(str).tolist()]\n",
        "dem_map = {str(k).strip(): dem_df.loc[i, label_col] for i,k in dem_df[id_col].astype(str).items()}\n",
        "print(\"Dem keys sample:\", dem_keys[:8], \"count:\", len(dem_keys))\n",
        "\n",
        "# --- load misses dataframe produced earlier ---\n",
        "if not os.path.exists(FINAL_MISSES):\n",
        "    raise SystemExit(\"Missing file: \" + FINAL_MISSES + \" ‚Äî run earlier finalize step first.\")\n",
        "miss_df = pd.read_csv(FINAL_MISSES)\n",
        "# normalize npz_file column name\n",
        "if 'npz_file' not in miss_df.columns and 'file' in miss_df.columns:\n",
        "    miss_df = miss_df.rename(columns={'file':'npz_file'})\n",
        "if 'npz_sid' not in miss_df.columns:\n",
        "    # try extracting sid from filename\n",
        "    def sid_from_fname(fname):\n",
        "        bn = os.path.basename(fname)\n",
        "        m = re.search(r'(subj[_-]\\d+|sub[_-]\\d+|subj\\d+|sub\\d+|\\d{2,})', bn, flags=re.IGNORECASE)\n",
        "        return (m.group(0) if m else os.path.splitext(bn)[0])\n",
        "    miss_df['npz_sid'] = miss_df['npz_file'].apply(lambda p: sid_from_fname(p) if isinstance(p, str) else \"\")\n",
        "\n",
        "print(\"Miss rows:\", len(miss_df))\n",
        "\n",
        "# candidate generator (common variants)\n",
        "def candidate_variants(sid):\n",
        "    sid = str(sid).strip()\n",
        "    variants = set()\n",
        "    if not sid: return []\n",
        "    variants.add(sid)\n",
        "    # numeric zero-pad forms\n",
        "    digits = re.sub(r'\\D+', '', sid)\n",
        "    if digits:\n",
        "        variants.add(digits)\n",
        "        variants.add(digits.zfill(4))\n",
        "        variants.add(\"OAS2_\" + digits.zfill(4))\n",
        "        variants.add(\"OAS2-\" + digits.zfill(4))\n",
        "        variants.add(\"OAS2\" + digits.zfill(4))\n",
        "    # uppercase/lowercase, prefixes, leading zeros\n",
        "    variants.add(sid.upper())\n",
        "    variants.add(sid.replace(\"subj\",\"OAS2_\").upper())\n",
        "    variants.add(\"OAS2_\" + sid)\n",
        "    # strip leading zeros\n",
        "    variants.add(sid.lstrip(\"0\"))\n",
        "    return [v for v in variants if v]\n",
        "\n",
        "# fuzzy similarity wrapper (difflib SequenceMatcher -> [0,1])\n",
        "def sim(a,b):\n",
        "    try:\n",
        "        return difflib.SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "rows_out = []\n",
        "auto_applied = []\n",
        "for _, r in tqdm(miss_df.iterrows(), total=len(miss_df), desc=\"Fuzzy matching\"):\n",
        "    npz_file = r.get('npz_file')\n",
        "    sid = str(r.get('npz_sid') or \"\")\n",
        "    cand_list = []\n",
        "    # first try exact variant matches\n",
        "    for v in candidate_variants(sid):\n",
        "        if v in dem_map:\n",
        "            cand_list.append((v, 1.0))\n",
        "    # if none exact, compute fuzzy top-K vs dem_keys (but compute against reasonable subset)\n",
        "    if not cand_list:\n",
        "        # reduce search set: dem_keys that contain any digits from sid or share tokens with sid\n",
        "        subset = dem_keys\n",
        "        digits = re.sub(r'\\D+','', sid)\n",
        "        if digits:\n",
        "            subset = [k for k in dem_keys if digits in re.sub(r'\\D+','',k)]\n",
        "        # if subset empty or too small, fallback to full dem_keys\n",
        "        if not subset or len(subset) < 10:\n",
        "            subset = dem_keys\n",
        "        # compute top-K\n",
        "        sims = [(k, sim(sid, k)) for k in subset]\n",
        "        sims = sorted(sims, key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "        cand_list.extend(sims)\n",
        "    # dedupe and keep top-K by score\n",
        "    seenk = set(); final_cands = []\n",
        "    for k,score in cand_list:\n",
        "        if k in seenk: continue\n",
        "        seenk.add(k)\n",
        "        final_cands.append((k, float(score)))\n",
        "    final_cands = sorted(final_cands, key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "    rows_out.append({\"npz_file\": npz_file, \"npz_sid\": sid, \"candidates\": \";\".join([f\"{k}|{s:.3f}\" for k,s in final_cands])})\n",
        "\n",
        "    # auto-apply if high confidence\n",
        "    if AUTO_APPLY and final_cands:\n",
        "        best_key, best_score = final_cands[0]\n",
        "        if best_score >= AUTO_THRESHOLD:\n",
        "            try:\n",
        "                arr = np.load(npz_file, allow_pickle=True)\n",
        "                x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([]))\n",
        "                ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "                y = np.array([float(dem_map[best_key])])\n",
        "                outp = os.path.join(LABELED_DIR, os.path.basename(npz_file).replace(\".npz\",\"_labeled.npz\"))\n",
        "                np.savez_compressed(outp, x=x, pos=pos, edge_index=ei, y=y)\n",
        "                auto_applied.append({\"npz_file\": npz_file, \"dem_key\": best_key, \"score\": best_score, \"out\": outp})\n",
        "            except Exception as e:\n",
        "                print(\"Auto-apply failed for\", npz_file, e)\n",
        "\n",
        "# save suggestions CSV\n",
        "sugg_df = pd.DataFrame(rows_out)\n",
        "sugg_df.to_csv(OUT_SUGGESTIONS, index=False)\n",
        "print(\"Saved fuzzy suggestions to:\", OUT_SUGGESTIONS)\n",
        "if PRINT_SAMPLE:\n",
        "    print(\"\\nSample suggestions (first 8):\")\n",
        "    print(sugg_df.head(8).to_dict(orient='records'))\n",
        "\n",
        "# if auto-applied, update index and misses\n",
        "if AUTO_APPLY and auto_applied:\n",
        "    print(\"Auto-applied mappings:\", len(auto_applied))\n",
        "    # load existing final index\n",
        "    idx = pd.read_csv(FINAL_INDEX) if os.path.exists(FINAL_INDEX) else pd.DataFrame(columns=[\"npz_file\",\"labeled_npz\",\"npz_sid\",\"dem_key\",\"y\"])\n",
        "    for a in auto_applied:\n",
        "        idx = idx.append({\"npz_file\": a['npz_file'], \"labeled_npz\": a['out'], \"npz_sid\": re.sub(r'\\D+','', os.path.basename(a['npz_file'])), \"dem_key\": a['dem_key'], \"y\": float(dem_map[a['dem_key']])}, ignore_index=True)\n",
        "    idx.to_csv(OUT_UPDATED_INDEX, index=False)\n",
        "    # recompute misses that remain (those in miss_df not auto_applied)\n",
        "    applied_set = set([a['npz_file'] for a in auto_applied])\n",
        "    remain = [row for row in rows_out if row['npz_file'] not in applied_set]\n",
        "    pd.DataFrame(remain).to_csv(OUT_UPDATED_MISSES, index=False)\n",
        "    print(\"Wrote updated index:\", OUT_UPDATED_INDEX, \"updated misses:\", OUT_UPDATED_MISSES)\n",
        "else:\n",
        "    print(\"AUTO_APPLY disabled or no auto mappings applied. Review suggestions CSV and either (A) set AUTO_APPLY=True and re-run, or (B) manually edit the suggestions CSV to pick mapped_dem_key and run the manual apply block from earlier.\")\n",
        "\n",
        "print(\"\\nDone. If you'd like I can also (1) re-run with a more permissive fuzzy method (Levenshtein) or (2) prepare a small interactive review table to let you pick matches in the notebook. Tell me which you prefer.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ikQMHHoUBWq",
        "outputId": "55500bc1-fee4-429c-c8ca-7cf653942e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dem keys sample: ['OAS2_0001', 'OAS2_0001', 'OAS2_0002', 'OAS2_0002', 'OAS2_0002', 'OAS2_0004', 'OAS2_0004', 'OAS2_0005'] count: 373\n",
            "Miss rows: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fuzzy matching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [00:00<00:00, 216.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fuzzy suggestions to: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions.csv\n",
            "\n",
            "Sample suggestions (first 8):\n",
            "[{'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0000_subj_0.npz', 'npz_sid': '', 'candidates': 'OAS2_0001|0.000;OAS2_0002|0.000'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0003_subj_3.npz', 'npz_sid': '3', 'candidates': 'OAS2_0013|0.200;OAS2_0023|0.200'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0006_subj_6.npz', 'npz_sid': '6', 'candidates': 'OAS2_0016|0.200;OAS2_0026|0.200;OAS2_0036|0.200'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0011_subj_11.npz', 'npz_sid': '11', 'candidates': 'OAS2_0111|0.364;OAS2_0112|0.364;OAS2_0113|0.364'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0015_subj_15.npz', 'npz_sid': '15', 'candidates': 'OAS2_0150|0.364;OAS2_0152|0.364'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0019_subj_19.npz', 'npz_sid': '19', 'candidates': 'OAS2_0109|0.364;OAS2_0119|0.364'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0024_subj_24.npz', 'npz_sid': '24', 'candidates': 'OAS2_0004|0.364;OAS2_0014|0.364;OAS2_0034|0.364'}, {'npz_file': '/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0025_subj_25.npz', 'npz_sid': '25', 'candidates': 'OAS2_0005|0.364;OAS2_0035|0.364'}]\n",
            "AUTO_APPLY disabled or no auto mappings applied. Review suggestions CSV and either (A) set AUTO_APPLY=True and re-run, or (B) manually edit the suggestions CSV to pick mapped_dem_key and run the manual apply block from earlier.\n",
            "\n",
            "Done. If you'd like I can also (1) re-run with a more permissive fuzzy method (Levenshtein) or (2) prepare a small interactive review table to let you pick matches in the notebook. Tell me which you prefer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERACTIVE REVIEW + APPLY (one cell)\n",
        "# - recomputes better fuzzy scores (rapidfuzz) and rewrites suggestions CSV\n",
        "# - lets you interactively accept a mapping for any miss row by index\n",
        "# - saves labeled npz + updated index + updated misses\n",
        "import os, glob, re, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
        "SUGG_CSV = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_fuzzy_suggestions.csv\")\n",
        "NPZ_DIR = os.path.join(BASE, \"data\", \"graphs\", \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
        "INDEX_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_graph_label_index_auto.csv\")\n",
        "OUT_SUGG_UPDATED = SUGG_CSV.replace(\".csv\",\"_rf.csv\")\n",
        "\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "# install rapidfuzz if available / needed\n",
        "try:\n",
        "    from rapidfuzz import fuzz, process\n",
        "except Exception:\n",
        "    print(\"Installing rapidfuzz...\")\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz import fuzz, process\n",
        "\n",
        "# load demographics and build map\n",
        "if not os.path.exists(DEM_PATH):\n",
        "    raise SystemExit(f\"Demographics file not found: {DEM_PATH}\")\n",
        "if DEM_PATH.lower().endswith('.csv'):\n",
        "    dem_df = pd.read_csv(DEM_PATH)\n",
        "else:\n",
        "    dem_df = pd.read_excel(DEM_PATH)\n",
        "dem_df.rename(columns={c:c.strip() for c in dem_df.columns}, inplace=True)\n",
        "cols = list(dem_df.columns)\n",
        "id_col = next((c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), cols[0])\n",
        "label_col = next((c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), None)\n",
        "if label_col is None:\n",
        "    numeric_cols = [c for c in cols if np.issubdtype(dem_df[c].dtype, np.number)]\n",
        "    label_col = numeric_cols[0] if numeric_cols else cols[-1]\n",
        "dem_keys = [str(x).strip() for x in dem_df[id_col].astype(str).tolist()]\n",
        "dem_map = {k: dem_df.loc[i, label_col] for i,k in dem_df[id_col].astype(str).items()}\n",
        "\n",
        "print(\"Loaded demographics:\", DEM_PATH)\n",
        "print(\"id_col:\", id_col, \"label_col:\", label_col, \"dem keys:\", len(dem_keys))\n",
        "\n",
        "# load misses/suggestions\n",
        "if not os.path.exists(MISSES_CSV):\n",
        "    raise SystemExit(\"Missing misses CSV: \" + MISSES_CSV)\n",
        "miss_df = pd.read_csv(MISSES_CSV)\n",
        "if 'npz_file' not in miss_df.columns and 'file' in miss_df.columns:\n",
        "    miss_df = miss_df.rename(columns={'file':'npz_file'})\n",
        "if 'npz_sid' not in miss_df.columns:\n",
        "    # attempt to extract sid\n",
        "    def sid_from_fname(fname):\n",
        "        if not isinstance(fname, str): return \"\"\n",
        "        bn = os.path.basename(fname)\n",
        "        m = re.search(r'(OAS2[_-]?\\d+|subj[_-]?\\d+|sub[_-]?\\d+|\\d{2,})', bn, flags=re.IGNORECASE)\n",
        "        return m.group(0) if m else os.path.splitext(bn)[0]\n",
        "    miss_df['npz_sid'] = miss_df['npz_file'].apply(lambda p: sid_from_fname(p) if isinstance(p,str) else \"\")\n",
        "\n",
        "print(\"Miss rows loaded:\", len(miss_df))\n",
        "\n",
        "# recompute improved suggestions using rapidfuzz (top 6)\n",
        "from rapidfuzz import process as rf_process\n",
        "TOP_K = 6\n",
        "rows=[]\n",
        "for i,row in miss_df.iterrows():\n",
        "    npz_file = row['npz_file']\n",
        "    sid = str(row.get('npz_sid','') or \"\")\n",
        "    # candidate strings to try\n",
        "    candidates = set()\n",
        "    candidates.add(sid)\n",
        "    digits = re.sub(r'\\D+','', sid)\n",
        "    if digits:\n",
        "        candidates.add(digits)\n",
        "        candidates.add(digits.zfill(4))\n",
        "        candidates.add(\"OAS2_\" + digits.zfill(4))\n",
        "    candidates.update([sid.upper(), sid.lower(), sid.replace(\"subj\",\"\").replace(\"sub\",\"\")])\n",
        "    # perform rapidfuzz against dem_keys (restrict set if digits present)\n",
        "    pool = dem_keys\n",
        "    if digits:\n",
        "        subset = [k for k in dem_keys if digits in re.sub(r'\\D+','',k)]\n",
        "        if subset: pool = subset\n",
        "    matches = rf_process.extract(str(sid), pool, scorer=fuzz.WRatio, limit=TOP_K)\n",
        "    matches = [(m[0], float(m[1]/100.0)) for m in matches]  # convert 0-100 -> 0-1\n",
        "    rows.append({\"idx\":int(i), \"npz_file\":npz_file, \"npz_sid\":sid, \"candidates\": \";\".join([f\"{k}|{s:.3f}\" for k,s in matches])})\n",
        "sugg_df = pd.DataFrame(rows)\n",
        "sugg_df.to_csv(OUT_SUGG_UPDATED, index=False)\n",
        "print(\"Wrote re-scored suggestions (rapidfuzz) ->\", OUT_SUGG_UPDATED)\n",
        "display(sugg_df.head(8))\n",
        "\n",
        "# helper: show one miss with candidate list (index from miss_df)\n",
        "def show_miss(idx):\n",
        "    if idx not in miss_df.index:\n",
        "        print(\"Index not in misses. Use a numeric index from 0..\", len(miss_df)-1)\n",
        "        return\n",
        "    row = miss_df.loc[idx]\n",
        "    npz_file = row['npz_file']\n",
        "    sid = str(row['npz_sid'] or \"\")\n",
        "    print(f\"MISS idx={idx}  npz_file={npz_file}  npz_sid={sid}\")\n",
        "    entry = sugg_df[sugg_df['idx']==idx]\n",
        "    if entry.empty:\n",
        "        print(\"No suggestion row (unexpected).\")\n",
        "        return\n",
        "    cand_str = entry.iloc[0]['candidates']\n",
        "    cand_list = []\n",
        "    for part in cand_str.split(\";\"):\n",
        "        if not part: continue\n",
        "        k,s = part.split(\"|\")\n",
        "        cand_list.append((k.strip(), float(s)))\n",
        "    if not cand_list:\n",
        "        print(\"No candidates found.\")\n",
        "        return\n",
        "    print(\"Top candidates (score 0..1):\")\n",
        "    for i,(k,s) in enumerate(cand_list):\n",
        "        print(f\"  [{i}] {k}   score={s:.3f}   label={dem_map.get(k,'<not-in-dem>')}\")\n",
        "    print(\"\\nTo accept a candidate, call: review_and_apply(idx, pick) where pick is candidate index (0..). Example: review_and_apply(3, 0)\")\n",
        "\n",
        "# helper: apply chosen mapping\n",
        "def review_and_apply(idx, pick):\n",
        "    idx = int(idx)\n",
        "    # validate\n",
        "    row = miss_df.loc[idx]\n",
        "    npz_file = row['npz_file']\n",
        "    entry = sugg_df[sugg_df['idx']==idx]\n",
        "    if entry.empty:\n",
        "        print(\"No suggestion row for idx\", idx); return\n",
        "    cand_str = entry.iloc[0]['candidates']\n",
        "    cand = [p for p in cand_str.split(\";\") if p]\n",
        "    if pick < 0 or pick >= len(cand):\n",
        "        print(\"pick out of range:\", pick); return\n",
        "    key, score = cand[pick].split(\"|\")\n",
        "    key = key.strip(); score = float(score)\n",
        "    print(f\"Applying mapping: {os.path.basename(npz_file)} -> {key} (score={score:.3f})\")\n",
        "    # load npz and write labeled npz\n",
        "    arr = np.load(npz_file, allow_pickle=True)\n",
        "    x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([]))\n",
        "    ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "    val = dem_map.get(key)\n",
        "    if val is None:\n",
        "        print(\"Selected dem key not in dem_map (abort).\")\n",
        "        return\n",
        "    y = np.array([float(val)])\n",
        "    outname = os.path.join(LABELED_DIR, os.path.basename(npz_file).replace(\".npz\",\"_labeled.npz\"))\n",
        "    np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "    print(\"Saved labeled npz ->\", outname)\n",
        "    # append to index CSV\n",
        "    idx_row = {\"npz_file\": npz_file, \"labeled_npz\": outname, \"npz_sid\": row.get('npz_sid'), \"dem_key\": key, \"y\": float(val)}\n",
        "    # load or create\n",
        "    if os.path.exists(INDEX_CSV):\n",
        "        index_df = pd.read_csv(INDEX_CSV)\n",
        "    else:\n",
        "        index_df = pd.DataFrame(columns=list(idx_row.keys()))\n",
        "    index_df = index_df.append(idx_row, ignore_index=True)\n",
        "    index_df.to_csv(INDEX_CSV, index=False)\n",
        "    print(\"Appended to index CSV:\", INDEX_CSV)\n",
        "    # remove row from misses and save updated misses CSV\n",
        "    new_miss_df = miss_df.drop(index=idx)\n",
        "    new_miss_df.to_csv(MISSES_CSV, index=False)\n",
        "    print(\"Removed from misses and saved updated misses CSV:\", MISSES_CSV)\n",
        "    # refresh in-memory miss_df and sugg_df (note: this does not mutate outer miss_df variable; re-run cell to refresh if needed)\n",
        "    print(\"DONE. If you want to continue, call show_miss(idx2) and review_and_apply(idx2, pick).\")\n",
        "\n",
        "# quick usage examples (do not auto-run mapping)\n",
        "print(\"\\nREADY. Examples:\")\n",
        "print(\"  show_miss(0)           # print first miss + candidates\")\n",
        "print(\"  review_and_apply(0,0)  # accept candidate 0 for miss 0 (writes labeled npz + updates CSVs)\\n\")\n",
        "\n",
        "# display a short table of all misses with their top candidate\n",
        "summary = sugg_df.copy()\n",
        "summary['top_candidate'] = summary['candidates'].str.split(\";\").str[0].fillna(\"\")\n",
        "display(summary[['idx','npz_file','npz_sid','top_candidate']].head(12))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "JbHNja-stGSj",
        "outputId": "ad223276-585f-4357-d476-e2eb68259535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing rapidfuzz...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoaded demographics: /content/drive/MyDrive/oasis_project/data/demographics/oasis2_demographics.xlsx\n",
            "id_col: Subject ID label_col: CDR dem keys: 373\n",
            "Miss rows loaded: 59\n",
            "Wrote re-scored suggestions (rapidfuzz) -> /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   idx                                           npz_file npz_sid  \\\n",
              "0    0  /content/drive/MyDrive/oasis_project/data/grap...           \n",
              "1    1  /content/drive/MyDrive/oasis_project/data/grap...       3   \n",
              "2    2  /content/drive/MyDrive/oasis_project/data/grap...       6   \n",
              "3    3  /content/drive/MyDrive/oasis_project/data/grap...      11   \n",
              "4    4  /content/drive/MyDrive/oasis_project/data/grap...      15   \n",
              "5    5  /content/drive/MyDrive/oasis_project/data/grap...      19   \n",
              "6    6  /content/drive/MyDrive/oasis_project/data/grap...      24   \n",
              "7    7  /content/drive/MyDrive/oasis_project/data/grap...      25   \n",
              "\n",
              "                                          candidates  \n",
              "0  OAS2_0001|0.000;OAS2_0001|0.000;OAS2_0002|0.00...  \n",
              "1  OAS2_0013|0.600;OAS2_0013|0.600;OAS2_0013|0.60...  \n",
              "2  OAS2_0016|0.600;OAS2_0016|0.600;OAS2_0026|0.60...  \n",
              "3  OAS2_0111|0.900;OAS2_0111|0.900;OAS2_0112|0.90...  \n",
              "4  OAS2_0150|0.900;OAS2_0150|0.900;OAS2_0152|0.90...  \n",
              "5    OAS2_0119|0.900;OAS2_0119|0.900;OAS2_0119|0.900  \n",
              "6                    OAS2_0124|0.900;OAS2_0124|0.900  \n",
              "7  OAS2_0002|0.600;OAS2_0002|0.600;OAS2_0002|0.60...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0bc947f-85c2-4413-bec7-bea4794c96f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>npz_file</th>\n",
              "      <th>npz_sid</th>\n",
              "      <th>candidates</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td></td>\n",
              "      <td>OAS2_0001|0.000;OAS2_0001|0.000;OAS2_0002|0.00...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>3</td>\n",
              "      <td>OAS2_0013|0.600;OAS2_0013|0.600;OAS2_0013|0.60...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>6</td>\n",
              "      <td>OAS2_0016|0.600;OAS2_0016|0.600;OAS2_0026|0.60...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>11</td>\n",
              "      <td>OAS2_0111|0.900;OAS2_0111|0.900;OAS2_0112|0.90...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>15</td>\n",
              "      <td>OAS2_0150|0.900;OAS2_0150|0.900;OAS2_0152|0.90...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>19</td>\n",
              "      <td>OAS2_0119|0.900;OAS2_0119|0.900;OAS2_0119|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>24</td>\n",
              "      <td>OAS2_0124|0.900;OAS2_0124|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>25</td>\n",
              "      <td>OAS2_0002|0.600;OAS2_0002|0.600;OAS2_0002|0.60...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0bc947f-85c2-4413-bec7-bea4794c96f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0bc947f-85c2-4413-bec7-bea4794c96f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0bc947f-85c2-4413-bec7-bea4794c96f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3dbc3d75-9d44-4f01-adf3-454c0d9fb303\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3dbc3d75-9d44-4f01-adf3-454c0d9fb303')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3dbc3d75-9d44-4f01-adf3-454c0d9fb303 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(summary[['idx','npz_file','npz_sid','top_candidate']]\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1,\n          5,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"npz_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0003_subj_3.npz\",\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0019_subj_19.npz\",\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0000_subj_0.npz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"npz_sid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"3\",\n          \"19\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"candidates\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"OAS2_0013|0.600;OAS2_0013|0.600;OAS2_0013|0.600;OAS2_0023|0.600;OAS2_0023|0.600;OAS2_0030|0.600\",\n          \"OAS2_0119|0.900;OAS2_0119|0.900;OAS2_0119|0.900\",\n          \"OAS2_0001|0.000;OAS2_0001|0.000;OAS2_0002|0.000;OAS2_0002|0.000;OAS2_0002|0.000;OAS2_0004|0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "READY. Examples:\n",
            "  show_miss(0)           # print first miss + candidates\n",
            "  review_and_apply(0,0)  # accept candidate 0 for miss 0 (writes labeled npz + updates CSVs)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    idx                                           npz_file npz_sid  \\\n",
              "0     0  /content/drive/MyDrive/oasis_project/data/grap...           \n",
              "1     1  /content/drive/MyDrive/oasis_project/data/grap...       3   \n",
              "2     2  /content/drive/MyDrive/oasis_project/data/grap...       6   \n",
              "3     3  /content/drive/MyDrive/oasis_project/data/grap...      11   \n",
              "4     4  /content/drive/MyDrive/oasis_project/data/grap...      15   \n",
              "5     5  /content/drive/MyDrive/oasis_project/data/grap...      19   \n",
              "6     6  /content/drive/MyDrive/oasis_project/data/grap...      24   \n",
              "7     7  /content/drive/MyDrive/oasis_project/data/grap...      25   \n",
              "8     8  /content/drive/MyDrive/oasis_project/data/grap...      33   \n",
              "9     9  /content/drive/MyDrive/oasis_project/data/grap...      38   \n",
              "10   10  /content/drive/MyDrive/oasis_project/data/grap...      59   \n",
              "11   11  /content/drive/MyDrive/oasis_project/data/grap...      65   \n",
              "\n",
              "      top_candidate  \n",
              "0   OAS2_0001|0.000  \n",
              "1   OAS2_0013|0.600  \n",
              "2   OAS2_0016|0.600  \n",
              "3   OAS2_0111|0.900  \n",
              "4   OAS2_0150|0.900  \n",
              "5   OAS2_0119|0.900  \n",
              "6   OAS2_0124|0.900  \n",
              "7   OAS2_0002|0.600  \n",
              "8   OAS2_0133|0.900  \n",
              "9   OAS2_0138|0.900  \n",
              "10  OAS2_0159|0.900  \n",
              "11  OAS2_0165|0.900  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caebcd93-fc50-4007-942e-a057e8d5f5c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>npz_file</th>\n",
              "      <th>npz_sid</th>\n",
              "      <th>top_candidate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td></td>\n",
              "      <td>OAS2_0001|0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>3</td>\n",
              "      <td>OAS2_0013|0.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>6</td>\n",
              "      <td>OAS2_0016|0.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>11</td>\n",
              "      <td>OAS2_0111|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>15</td>\n",
              "      <td>OAS2_0150|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>19</td>\n",
              "      <td>OAS2_0119|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>24</td>\n",
              "      <td>OAS2_0124|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>25</td>\n",
              "      <td>OAS2_0002|0.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>33</td>\n",
              "      <td>OAS2_0133|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>38</td>\n",
              "      <td>OAS2_0138|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>59</td>\n",
              "      <td>OAS2_0159|0.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/grap...</td>\n",
              "      <td>65</td>\n",
              "      <td>OAS2_0165|0.900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caebcd93-fc50-4007-942e-a057e8d5f5c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-caebcd93-fc50-4007-942e-a057e8d5f5c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-caebcd93-fc50-4007-942e-a057e8d5f5c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9823bb87-c5dc-4a1b-a849-99e6e017279b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9823bb87-c5dc-4a1b-a849-99e6e017279b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9823bb87-c5dc-4a1b-a849-99e6e017279b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(summary[['idx','npz_file','npz_sid','top_candidate']]\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          10,\n          9,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"npz_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0059_subj_59.npz\",\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0038_subj_38.npz\",\n          \"/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0000_subj_0.npz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"npz_sid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"59\",\n          \"38\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_candidate\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"OAS2_0159|0.900\",\n          \"OAS2_0138|0.900\",\n          \"OAS2_0001|0.000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE CELL: interactive review + apply fuzzy suggestions + finalize .pt\n",
        "import os, glob, json, numpy as np, pandas as pd, shutil, re\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML\n",
        "from collections import OrderedDict\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "NPZ_DIR = os.path.join(BASE, \"data\", \"graphs\", \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(BASE, \"data\", \"graphs\", \"labeled_npz_auto\")\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "INDEX_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_graph_label_index_auto.csv\")\n",
        "MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"final_label_misses_auto.csv\")\n",
        "FUZZY_CSV_RF = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_fuzzy_suggestions_rf.csv\")\n",
        "FUZZY_CSV = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_fuzzy_suggestions.csv\")\n",
        "AUTO_INDEX_CSV = os.path.join(BASE, \"data\", \"graphs\", \"graph_label_index_auto.csv\")\n",
        "AUTO_MISSES_CSV = os.path.join(BASE, \"data\", \"graphs\", \"label_misses_auto.csv\")\n",
        "\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "# Helper: try to load the best fuzzy suggestions CSV that exists\n",
        "fuzzy_candidates = [FUZZY_CSV_RF, FUZZY_CSV, AUTO_MISSES_CSV, MISSES_CSV]\n",
        "fuzzy_path = next((p for p in fuzzy_candidates if os.path.exists(p)), None)\n",
        "\n",
        "if fuzzy_path is None:\n",
        "    raise SystemExit(\"No fuzzy/misses CSV found. Look for files like label_misses_fuzzy_suggestions_rf.csv or final_label_misses_auto.csv in data/graphs/\")\n",
        "\n",
        "print(\"Using fuzzy/misses file:\", fuzzy_path)\n",
        "\n",
        "# Load fuzzy suggestions (expected columns: npz_file, npz_sid, candidates or suggestions)\n",
        "fdf = pd.read_csv(fuzzy_path)\n",
        "# normalize columns\n",
        "if 'candidates' not in fdf.columns and 'suggestions' in fdf.columns:\n",
        "    fdf = fdf.rename(columns={'suggestions': 'candidates'})\n",
        "\n",
        "# If candidates are stored as semicolon-delimited 'key|score;key|score' strings\n",
        "def parse_candidates(cell):\n",
        "    if pd.isna(cell): return []\n",
        "    if isinstance(cell, (list,tuple)): return list(cell)\n",
        "    s = str(cell)\n",
        "    # sometimes stored as JSON list of dicts\n",
        "    try:\n",
        "        parsed = json.loads(s)\n",
        "        if isinstance(parsed, list):\n",
        "            # try to convert list of dicts -> list of \"key|score\"\n",
        "            out=[]\n",
        "            for item in parsed:\n",
        "                if isinstance(item, dict):\n",
        "                    k = next(iter(item.values())) if len(item)>0 else None\n",
        "                    # fallback: create representation\n",
        "                    out.append(json.dumps(item))\n",
        "                else:\n",
        "                    out.append(str(item))\n",
        "            return out\n",
        "    except Exception:\n",
        "        pass\n",
        "    parts = []\n",
        "    # split by ; or |\n",
        "    # detect patterns like \"OAS2_0111|0.900;OAS2_0112|0.900\"\n",
        "    for token in re.split(r'[;\\\\n]+', s):\n",
        "        token = token.strip()\n",
        "        if not token: continue\n",
        "        parts.append(token)\n",
        "    return parts\n",
        "\n",
        "# Create an actionable dataframe: each row has npz_file, npz_sid, candidates(list of (key,score))\n",
        "rows = []\n",
        "for i, row in fdf.iterrows():\n",
        "    npz_file = row.get('npz_file') if 'npz_file' in row else row.get('file') if 'file' in row else None\n",
        "    npz_sid = row.get('npz_sid') if 'npz_sid' in row else ''\n",
        "    cand_cell = row.get('candidates') if 'candidates' in row else row.get('suggestions') if 'suggestions' in row else ''\n",
        "    cand_list = parse_candidates(cand_cell)\n",
        "    # transform cand_list items into (key,score) tuples\n",
        "    parsed = []\n",
        "    for item in cand_list:\n",
        "        item = str(item)\n",
        "        # if JSON-like dict string, try to extract 'key' and 'score'\n",
        "        if '|' in item and item.count('|')>=1:\n",
        "            parts = item.split('|')\n",
        "            key = parts[0].strip()\n",
        "            try:\n",
        "                score = float(parts[1])\n",
        "            except:\n",
        "                score = None\n",
        "            parsed.append((key, score))\n",
        "        else:\n",
        "            # sometimes item like \"OAS2_0001\" or \"OAS2_0001:0.9\"\n",
        "            m = re.match(r'^(?P<k>[A-Za-z0-9_\\-]+)[\\|: ]?(?P<s>[0-9\\.]+)?', item)\n",
        "            if m:\n",
        "                key = m.group('k')\n",
        "                s = m.group('s')\n",
        "                score = float(s) if s is not None else None\n",
        "                parsed.append((key, score))\n",
        "            else:\n",
        "                parsed.append((item, None))\n",
        "    rows.append({\"idx\": i, \"npz_file\": npz_file, \"npz_sid\": str(npz_sid), \"candidates\": parsed})\n",
        "\n",
        "acts = pd.DataFrame(rows)\n",
        "\n",
        "# Load demographics for label lookup\n",
        "if not os.path.exists(DEM_PATH):\n",
        "    print(\"Warning: demographics file not found at expected DEM_PATH:\", DEM_PATH)\n",
        "    dem_df = None\n",
        "else:\n",
        "    try:\n",
        "        dem_df = pd.read_excel(DEM_PATH)\n",
        "        dem_df.rename(columns={c:c.strip() for c in dem_df.columns}, inplace=True)\n",
        "        id_col = next((c for c in dem_df.columns if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), dem_df.columns[0])\n",
        "        lab_col = next((c for c in dem_df.columns if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), dem_df.columns[-1])\n",
        "        dem_map = {str(v).strip(): dem_df.iloc[i][lab_col] for i,v in enumerate(dem_df[id_col].astype(str))}\n",
        "        print(\"Loaded demographics: {}, id_col = {}, label_col = {}, dem entries = {}\".format(DEM_PATH, id_col, lab_col, len(dem_map)))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read demographics:\", e)\n",
        "        dem_df = None\n",
        "        dem_map = {}\n",
        "\n",
        "# Index / misses CSVs (create if missing)\n",
        "if not os.path.exists(INDEX_CSV):\n",
        "    idx_df = pd.DataFrame(columns=[\"subject_id\",\"source\",\"file\",\"n_nodes\",\"feat_dim\",\"y\"])\n",
        "    idx_df.to_csv(INDEX_CSV, index=False)\n",
        "else:\n",
        "    idx_df = pd.read_csv(INDEX_CSV)\n",
        "\n",
        "if not os.path.exists(MISSES_CSV):\n",
        "    misses_df = acts[['idx','npz_file','npz_sid']].rename(columns={'idx':'miss_idx'}).copy()\n",
        "    misses_df.to_csv(MISSES_CSV, index=False)\n",
        "else:\n",
        "    misses_df = pd.read_csv(MISSES_CSV)\n",
        "\n",
        "# utility: pretty print one miss\n",
        "def show_miss(i):\n",
        "    \"\"\"Display miss row i and candidate list with dem label if available.\"\"\"\n",
        "    row = acts[acts['idx']==i]\n",
        "    if row.empty:\n",
        "        print(f\"No miss with idx={i} (rows available: {acts['idx'].tolist()[:20]} ... )\")\n",
        "        return\n",
        "    row = row.iloc[0]\n",
        "    npzf = row['npz_file']; sid = row['npz_sid']; cands = row['candidates']\n",
        "    print(f\"MISS idx={i}\\n npz_file={npzf}\\n npz_sid='{sid}'\\nCandidates (best first):\")\n",
        "    for j, (key, score) in enumerate(cands):\n",
        "        label_val = dem_map.get(key) if dem_map else None\n",
        "        print(f\"  [{j}] {key}  score={score}  dem_label={label_val}\")\n",
        "    # also show a tiny preview of the NPZ's shapes if possible\n",
        "    if npzf and os.path.exists(npzf):\n",
        "        try:\n",
        "            arr = np.load(npzf, allow_pickle=True)\n",
        "            x = arr.get('x'); pos = arr.get('pos'); ei = arr.get('edge_index') or arr.get('edges') or arr.get('edge_idx')\n",
        "            print(\"NPZ preview shapes: x:\", getattr(x,'shape',None), \"pos:\", getattr(pos,'shape',None), \"edge_index:\", getattr(ei,'shape',None))\n",
        "        except Exception as e:\n",
        "            print(\"Could not load npz preview:\", e)\n",
        "    else:\n",
        "        print(\"NPZ file not found on disk.\")\n",
        "\n",
        "# utility: accept candidate j for miss i -> writes labeled NPZ and updates CSVs\n",
        "def review_and_apply(i, j):\n",
        "    \"\"\"Accept candidate j for miss index i. Writes labeled npz and updates CSVs.\"\"\"\n",
        "    row = acts[acts['idx']==i]\n",
        "    if row.empty:\n",
        "        print(\"No miss with idx=\", i); return\n",
        "    row = row.iloc[0]\n",
        "    npzf = row['npz_file']; sid = row['npz_sid']; cands = row['candidates']\n",
        "    if j<0 or j>=len(cands):\n",
        "        print(\"Candidate index out of range.\"); return\n",
        "    dem_key, score = cands[j]\n",
        "    # find label value from demographics\n",
        "    if dem_map and dem_key in dem_map:\n",
        "        yval = dem_map[dem_key]\n",
        "    else:\n",
        "        # try to match numeric part (like OAS2_0001 -> 0001)\n",
        "        digits = re.sub(r'\\D+','', dem_key)\n",
        "        yval = dem_map.get(dem_key) or dem_map.get(digits) or None\n",
        "    if yval is None:\n",
        "        print(\"WARNING: could not find dem label for key\", dem_key, \"‚Äî still proceeding but y will be saved as string.\")\n",
        "    # load npz\n",
        "    if not npzf or not os.path.exists(npzf):\n",
        "        print(\"NPZ file missing:\", npzf); return\n",
        "    arr = np.load(npzf, allow_pickle=True)\n",
        "    x = arr.get('x', np.array([]))\n",
        "    pos = arr.get('pos', np.array([]))\n",
        "    ei = arr.get('edge_index', arr.get('edges', arr.get('edge_idx', np.empty((2,0), dtype=np.int64))))\n",
        "    y = np.array([float(yval)]) if (yval is not None and (isinstance(yval, (int,float)) or (isinstance(yval, np.number)))) else np.array([yval])\n",
        "    # write labeled npz\n",
        "    outname = os.path.join(LABELED_DIR, os.path.basename(npzf).replace(\".npz\",\"_labeled.npz\"))\n",
        "    np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "    print(\"Wrote labeled npz:\", outname, \"mapped_dem_key:\", dem_key, \"score:\", score, \"y:\", y)\n",
        "    # update index CSV (append)\n",
        "    # compute n_nodes / feat_dim safely\n",
        "    try:\n",
        "        n_nodes = int(np.asarray(x).shape[0])\n",
        "        feat_dim = int(np.asarray(x).shape[1]) if np.asarray(x).ndim>1 else 1\n",
        "    except Exception:\n",
        "        n_nodes = 0; feat_dim = 0\n",
        "    newrow = {\"subject_id\": dem_key, \"source\": \"auto_fuzzy\", \"file\": outname, \"n_nodes\": n_nodes, \"feat_dim\": feat_dim, \"y\": float(y) if (isinstance(y, np.ndarray) and np.issubdtype(y.dtype, np.number)) else y}\n",
        "    # append to CSVs in memory and disk\n",
        "    global idx_df, misses_df\n",
        "    idx_df = pd.concat([idx_df, pd.DataFrame([newrow])], ignore_index=True)\n",
        "    idx_df.to_csv(INDEX_CSV, index=False)\n",
        "    # remove from misses: delete acts row and rewrite misses CSV\n",
        "    acts.drop(acts[acts['idx']==i].index, inplace=True)\n",
        "    # also remove entry from misses_df if exists (match by npz_file)\n",
        "    misses_df = misses_df[misses_df['npz_file'] != npzf]\n",
        "    misses_df.to_csv(MISSES_CSV, index=False)\n",
        "    # update fuzzy CSV file by removing that row\n",
        "    # read original CSV and drop matching npz_file row\n",
        "    try:\n",
        "        orig = pd.read_csv(fuzzy_path)\n",
        "        orig = orig[orig['npz_file'] != npzf]\n",
        "        orig.to_csv(fuzzy_path, index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # update index CSV on disk\n",
        "    print(\"Index CSV and misses CSV updated.\")\n",
        "    return outname\n",
        "\n",
        "# convenience: auto-apply highly confident suggestions\n",
        "def auto_apply_threshold(threshold=0.85, dry_run=False, max_apply=500):\n",
        "    \"\"\"Auto-apply candidate[0] for rows where candidate score >= threshold.\n",
        "       dry_run=True will just print; returns list of applied (idx,dem_key).\"\"\"\n",
        "    applied=[]\n",
        "    candidates_to_apply = []\n",
        "    for _, row in acts.iterrows():\n",
        "        i = row['idx']; cands = row['candidates']\n",
        "        if not cands: continue\n",
        "        key, score = cands[0]\n",
        "        score_val = score if score is not None else 0.0\n",
        "        if score_val >= threshold:\n",
        "            candidates_to_apply.append((i, key, score_val))\n",
        "    # sort by score desc\n",
        "    candidates_to_apply = sorted(candidates_to_apply, key=lambda x: -x[2])[:max_apply]\n",
        "    print(f\"Auto-applying {len(candidates_to_apply)} matches with score >= {threshold}. dry_run={dry_run}\")\n",
        "    for i,key,score in candidates_to_apply:\n",
        "        print(\" ->\", i, key, score)\n",
        "        if not dry_run:\n",
        "            # find candidate index in acts\n",
        "            row = acts[acts['idx']==i].iloc[0]\n",
        "            # find which candidate entry matches key\n",
        "            cand_idx = next((k for k,(kk,sc) in enumerate(row['candidates']) if kk==key), 0)\n",
        "            review_and_apply(i, cand_idx)\n",
        "            applied.append((i,key,score))\n",
        "    return applied\n",
        "\n",
        "# finalizer: build .pt from all labeled npz files in LABELED_DIR and save into oasis2_graphs_labeled_final.pt\n",
        "def build_final_pt(out_pt=None, overwrite=False):\n",
        "    \"\"\"Load all labeled npz in LABELED_DIR and save a single .pt list of torch_geometric.data.Data\"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        from torch_geometric.data import Data\n",
        "    except Exception as e:\n",
        "        print(\"Could not import torch/torch_geometric:\", e)\n",
        "        return\n",
        "    out_pt = out_pt or os.path.join(BASE, \"data\", \"graphs\", \"oasis2_graphs_labeled_final.pt\")\n",
        "    if os.path.exists(out_pt) and not overwrite:\n",
        "        print(\"Out .pt already exists:\", out_pt, \"use overwrite=True to replace.\")\n",
        "        return\n",
        "    npz_files = sorted(glob.glob(os.path.join(LABELED_DIR, \"*_labeled.npz\")))\n",
        "    if not npz_files:\n",
        "        print(\"No labeled npz files found in\", LABELED_DIR)\n",
        "        return\n",
        "    data_list = []\n",
        "    for p in npz_files:\n",
        "        try:\n",
        "            arr = np.load(p, allow_pickle=True)\n",
        "            x = arr.get('x', np.array([]))\n",
        "            pos = arr.get('pos', np.array([]))\n",
        "            ei = arr.get('edge_index', np.empty((2,0),dtype=np.int64))\n",
        "            y = arr.get('y', np.array([]))\n",
        "            xt = torch.tensor(x, dtype=torch.float) if getattr(x,'size', lambda:0)() else None\n",
        "            post = torch.tensor(pos, dtype=torch.float) if getattr(pos,'size', lambda:0)() else None\n",
        "            eit = torch.tensor(ei, dtype=torch.long) if getattr(ei,'size', lambda:0)() else None\n",
        "            yt = torch.tensor(y, dtype=torch.float) if getattr(y,'size', lambda:0)() else None\n",
        "            g = Data(x=xt, edge_index=eit, pos=post, y=yt)\n",
        "            # subject id from filename or stored in y/npz name\n",
        "            subj = os.path.splitext(os.path.basename(p))[0].replace(\"_labeled\",\"\")\n",
        "            g.subject_id = subj\n",
        "            data_list.append(g)\n",
        "        except Exception as e:\n",
        "            print(\"Skipping\", p, \"due to error:\", e)\n",
        "    if not data_list:\n",
        "        print(\"No Data objects constructed.\")\n",
        "        return\n",
        "    torch.save(data_list, out_pt)\n",
        "    print(\"Saved final labeled .pt with\", len(data_list), \"graphs to:\", out_pt)\n",
        "    return out_pt\n",
        "\n",
        "# Print small usage instruction\n",
        "print(\"\\nREADY. Usage examples:\")\n",
        "print(\"  show_miss(3)             # inspect miss index 3\")\n",
        "print(\"  review_and_apply(3, 0)   # accept candidate 0 for miss 3 (writes labeled npz + updates CSVs)\")\n",
        "print(\"  auto_apply_threshold(0.9) # auto accept high-confidence matches (dry_run default False)\")\n",
        "print(\"  build_final_pt(overwrite=True)  # aggregate labeled npz -> single .pt\\n\")\n",
        "\n",
        "# expose a light HTML table for the first few misses\n",
        "try:\n",
        "    display(HTML(acts[['idx','npz_file','npz_sid']].head(12).to_html(index=False)))\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "xE0gpIIUtoy4",
        "outputId": "b86d30ac-192f-4cde-bb1b-94ef7e915152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using fuzzy/misses file: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "Loaded demographics: /content/drive/MyDrive/oasis_project/data/demographics/oasis2_demographics.xlsx, id_col = Subject ID, label_col = CDR, dem entries = 150\n",
            "\n",
            "READY. Usage examples:\n",
            "  show_miss(3)             # inspect miss index 3\n",
            "  review_and_apply(3, 0)   # accept candidate 0 for miss 3 (writes labeled npz + updates CSVs)\n",
            "  auto_apply_threshold(0.9) # auto accept high-confidence matches (dry_run default False)\n",
            "  build_final_pt(overwrite=True)  # aggregate labeled npz -> single .pt\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>npz_file</th>\n",
              "      <th>npz_sid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0000_subj_0.npz</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0003_subj_3.npz</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0006_subj_6.npz</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0011_subj_11.npz</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0015_subj_15.npz</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0019_subj_19.npz</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0024_subj_24.npz</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0025_subj_25.npz</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0033_subj_33.npz</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0038_subj_38.npz</td>\n",
              "      <td>38.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0059_subj_59.npz</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>/content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0065_subj_65.npz</td>\n",
              "      <td>65.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE-CELL TOOL: Export .pt -> npz, auto-match demographics with fuzzy suggestions,\n",
        "# interactive helpers: show_miss, review_and_apply, auto_apply_threshold, build_final_pt\n",
        "# Drop this into one notebook cell and run. It will:\n",
        "#  - find / recover the .pt dataset if possible\n",
        "#  - export graphs to NPZ (if .pt available)\n",
        "#  - load demographics (prompt looks in project dem folder or root drive)\n",
        "#  - perform strict + fuzzy matching to propose dem keys for each NPZ\n",
        "#  - provide helper functions to accept suggestions and build a final labeled .pt\n",
        "#\n",
        "# NOTE: This code is defensive and prints progress. Edit PATHS near top if needed.\n",
        "\n",
        "import os, glob, re, shutil, json, numpy as np, pandas as pd, traceback\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "\n",
        "# optional imports (faster fuzzy matching). Will fallback to difflib if missing.\n",
        "try:\n",
        "    from rapidfuzz import process as rf_process, fuzz as rf_fuzz\n",
        "    HAVE_RAPIDFUZZ = True\n",
        "except Exception:\n",
        "    import difflib\n",
        "    HAVE_RAPIDFUZZ = False\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "except Exception:\n",
        "    torch = None\n",
        "    Data = None\n",
        "\n",
        "# ---------------- CONFIG (change here if you want) ----------------\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "SRC_PT_CANON = os.path.join(BASE, \"oasis2_graph_dataset.pt\")   # canonical expected path\n",
        "OUTPUT_DIR = os.path.join(BASE, \"data\", \"graphs\")\n",
        "NPZ_OUT = os.path.join(OUTPUT_DIR, \"npz_graphs_resave\")\n",
        "LABELED_OUT = os.path.join(OUTPUT_DIR, \"labeled_npz_auto\")\n",
        "FINAL_PT = os.path.join(OUTPUT_DIR, \"oasis2_graphs_labeled_auto_final.pt\")\n",
        "DEM_PREF = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "\n",
        "# suggestion / index CSVs\n",
        "FUZZY_CSV = os.path.join(OUTPUT_DIR, \"label_misses_fuzzy_suggestions_rf.csv\")\n",
        "INDEX_CSV = os.path.join(OUTPUT_DIR, \"final_graph_label_index_auto.csv\")\n",
        "MISSES_CSV = os.path.join(OUTPUT_DIR, \"final_label_misses_auto.csv\")\n",
        "\n",
        "os.makedirs(NPZ_OUT, exist_ok=True)\n",
        "os.makedirs(LABELED_OUT, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def safe_load_torch(path):\n",
        "    \"\"\"Try to load a torch file safely; map storages to cpu if needed.\"\"\"\n",
        "    if torch is None:\n",
        "        raise RuntimeError(\"torch not available in environment.\")\n",
        "    try:\n",
        "        return torch.load(path, weights_only=False)\n",
        "    except RuntimeError as e:\n",
        "        # try mapping to CPU (common when checkpoint saved on CUDA)\n",
        "        try:\n",
        "            return torch.load(path, map_location=torch.device('cpu'), weights_only=False)\n",
        "        except Exception:\n",
        "            raise\n",
        "\n",
        "def find_candidate_pt():\n",
        "    \"\"\"Search known locations for a dataset .pt containing list-of-graphs.\"\"\"\n",
        "    # candidates: canonical path, outputs folder copies\n",
        "    cands = []\n",
        "    if os.path.exists(SRC_PT_CANON):\n",
        "        cands.append(SRC_PT_CANON)\n",
        "    out_dir = os.path.join(BASE, \"outputs\")\n",
        "    if os.path.isdir(out_dir):\n",
        "        for p in sorted(glob.glob(os.path.join(out_dir, \"*.pt\"))):\n",
        "            cands.append(p)\n",
        "    # also check top-level for anything named oasis*dataset*.pt\n",
        "    for p in glob.glob(os.path.join(BASE, \"**\", \"*graph_dataset*.pt\"), recursive=True):\n",
        "        cands.append(p)\n",
        "    # de-duplicate preserving order\n",
        "    seen = set(); uniq=[]\n",
        "    for p in cands:\n",
        "        if p not in seen and os.path.exists(p):\n",
        "            uniq.append(p); seen.add(p)\n",
        "    return uniq\n",
        "\n",
        "def export_pt_to_npz(pt_path, npz_out=NPZ_OUT, overwrite=False):\n",
        "    \"\"\"Load a .pt list-of-Data and export each graph to compressed npz. Returns list of npz paths.\"\"\"\n",
        "    print(\"Exporting .pt -> npz from:\", pt_path)\n",
        "    raw = safe_load_torch(pt_path)\n",
        "    if not isinstance(raw, (list, tuple)):\n",
        "        raise RuntimeError(\"Loaded .pt is not a list/tuple of graphs. File: \" + pt_path)\n",
        "    graphs = list(raw)\n",
        "    print(\"Loaded graphs count:\", len(graphs))\n",
        "    out_files = []\n",
        "    for i,g in enumerate(graphs):\n",
        "        # robust extraction\n",
        "        sid = getattr(g, 'subject_id', None) or getattr(g, 'sid', None) or getattr(g, 'filename', None) or f\"subj_{i}\"\n",
        "        # get tensors/arrays to numpy\n",
        "        def to_np(v):\n",
        "            if v is None: return np.array([])\n",
        "            try:\n",
        "                return v.cpu().numpy()\n",
        "            except Exception:\n",
        "                return np.asarray(v)\n",
        "        x = to_np(getattr(g, 'x', None))\n",
        "        pos = to_np(getattr(g, 'pos', None))\n",
        "        # try multiple edge attr names\n",
        "        ei = None\n",
        "        for nm in ('edge_index','edges','edge_idx'):\n",
        "            if hasattr(g, nm):\n",
        "                ei = getattr(g, nm)\n",
        "                break\n",
        "        ei = to_np(ei)\n",
        "        # normalize edge shape\n",
        "        try:\n",
        "            ei = np.asarray(ei)\n",
        "            if ei.ndim == 1 and ei.size % 2 == 0:\n",
        "                ei = ei.reshape(2,-1)\n",
        "            elif ei.ndim == 2 and ei.shape[0] != 2 and ei.shape[1] == 2:\n",
        "                ei = ei.T\n",
        "            elif ei.ndim != 2:\n",
        "                ei = np.empty((2,0), dtype=np.int64)\n",
        "        except Exception:\n",
        "            ei = np.empty((2,0), dtype=np.int64)\n",
        "        safe_sid = re.sub(r'[^0-9A-Za-z_\\-\\.]', '_', str(sid))\n",
        "        outp = os.path.join(npz_out, f\"graph_{i:04d}_{safe_sid}.npz\")\n",
        "        if os.path.exists(outp) and not overwrite:\n",
        "            out_files.append(outp)\n",
        "            continue\n",
        "        np.savez_compressed(outp, x=x, pos=pos, edge_index=ei)\n",
        "        out_files.append(outp)\n",
        "    print(\"Exported npz examples:\", out_files[:3])\n",
        "    return out_files\n",
        "\n",
        "def find_demographics(pref=DEM_PREF):\n",
        "    \"\"\"Return path to demographics file (xlsx/csv) or None.\"\"\"\n",
        "    if pref and os.path.exists(pref):\n",
        "        return pref\n",
        "    # search typical places: project folder and MyDrive root\n",
        "    candidates=[]\n",
        "    for root in (BASE, os.path.join(\"/content/drive\",\"MyDrive\"), \"/content\"):\n",
        "        for ext in (\"*.xlsx\",\"*.xls\",\"*.csv\"):\n",
        "            for p in glob.glob(os.path.join(root, \"**\", ext), recursive=False):\n",
        "                candidates.append(p)\n",
        "    # deeper scan for oasis+dem names (limited)\n",
        "    for ext in (\"*.xlsx\",\"*.xls\",\"*.csv\"):\n",
        "        for p in glob.glob(os.path.join(\"/content/drive\",\"MyDrive\",\"**\", ext), recursive=True):\n",
        "            name = os.path.basename(p).lower()\n",
        "            if 'oasis' in name and ('dem' in name or 'demograph' in name or 'demographics' in name):\n",
        "                candidates.append(p)\n",
        "    candidates = [c for c in sorted(set(candidates)) if os.path.exists(c)]\n",
        "    return candidates[0] if candidates else None\n",
        "\n",
        "def read_dem(dfpath):\n",
        "    if dfpath.lower().endswith('.csv'):\n",
        "        df = pd.read_csv(dfpath)\n",
        "    else:\n",
        "        df = pd.read_excel(dfpath)\n",
        "    df.rename(columns={c:c.strip() for c in df.columns}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def detect_id_label_columns(df):\n",
        "    cols = list(df.columns)\n",
        "    id_candidates = [c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\",\"OASIS\"))]\n",
        "    if not id_candidates:\n",
        "        id_candidates = cols[:1]\n",
        "    id_col = id_candidates[0]\n",
        "    label_candidates = [c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))]\n",
        "    if not label_candidates:\n",
        "        numeric_cols = [c for c in cols if np.issubdtype(df[c].dtype, np.number)]\n",
        "        label_candidates = numeric_cols if numeric_cols else [cols[-1]]\n",
        "    label_col = label_candidates[0]\n",
        "    return id_col, label_col\n",
        "\n",
        "def normalize_dem_key(x):\n",
        "    if pd.isna(x): return None\n",
        "    s = str(x).strip()\n",
        "    # common dem keys look like OAS2_0123 or numeric 123; keep both forms as alternatives\n",
        "    return s\n",
        "\n",
        "# ---------------- core matching logic ----------------\n",
        "def build_dem_map(dem_path):\n",
        "    df = read_dem(dem_path)\n",
        "    id_col, label_col = detect_id_label_columns(df)\n",
        "    dem_map = {}\n",
        "    dem_keys = []\n",
        "    for _, r in df.iterrows():\n",
        "        key = normalize_dem_key(r.get(id_col))\n",
        "        if key is None: continue\n",
        "        dem_keys.append(key)\n",
        "        val = r.get(label_col)\n",
        "        dem_map[key] = val\n",
        "    dem_keys = sorted(list(set(dem_keys)))\n",
        "    return dem_map, dem_keys, id_col, label_col, df\n",
        "\n",
        "def extract_npz_sid(fname):\n",
        "    \"\"\"Extract the numeric-ish id from filename (commonly last token like subj_12 or 0012).\"\"\"\n",
        "    b = os.path.basename(fname)\n",
        "    m = re.search(r'([0-9]{2,}|subj[_-]?\\d+|sub[_-]?\\d+)', b, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        # fallback: basename without extension\n",
        "        return os.path.splitext(b)[0]\n",
        "    s = m.group(0)\n",
        "    # trim \"subj_\" -> keep numeric if possible\n",
        "    s2 = re.sub(r'^(subj[_-]?|sub[_-]?)', '', s, flags=re.IGNORECASE)\n",
        "    return s2\n",
        "\n",
        "def fuzzy_candidates_for_sid(sid, dem_keys, topk=5):\n",
        "    \"\"\"Return list of (dem_key, score) best matches for sid (score 0-1).\"\"\"\n",
        "    if sid is None or sid == \"\":\n",
        "        return []\n",
        "    sid_str = str(sid)\n",
        "    if HAVE_RAPIDFUZZ:\n",
        "        # rapidfuzz returns scores up to 100; prefer ratio on whole string\n",
        "        matches = rf_process.extract(sid_str, dem_keys, scorer=rf_fuzz.ratio, limit=topk)\n",
        "        # convert to 0..1 floats and return\n",
        "        return [(m[0], float(m[1]) / 100.0) for m in matches]\n",
        "    else:\n",
        "        # use difflib SequenceMatcher ratio\n",
        "        scored = []\n",
        "        for k in dem_keys:\n",
        "            score = difflib.SequenceMatcher(a=sid_str.lower(), b=str(k).lower()).ratio()\n",
        "            scored.append((k, score))\n",
        "        scored.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scored[:topk]\n",
        "\n",
        "# ---------------- RUN pipeline (export if needed) ----------------\n",
        "print(\"STARTING pipeline. Base:\", BASE)\n",
        "# 1) Ensure we have NPZs: attempt to use existing, else export from a pt if present\n",
        "npz_files = sorted(glob.glob(os.path.join(NPZ_OUT, \"*.npz\")))\n",
        "if len(npz_files) == 0:\n",
        "    print(\"No npz files in\", NPZ_OUT, \"-> searching for .pt dataset to export.\")\n",
        "    pts = find_candidate_pt()\n",
        "    if not pts:\n",
        "        print(\"‚ùå No .pt candidates found in project. If you have a .pt dataset upload it to\", BASE)\n",
        "    else:\n",
        "        recovered = False\n",
        "        for p in pts:\n",
        "            try:\n",
        "                print(\"Trying to load candidate .pt:\", p)\n",
        "                raw = safe_load_torch(p)\n",
        "                # if it's a list, we can export\n",
        "                if isinstance(raw, list):\n",
        "                    # if canonical not existing, copy to canonical for bookkeeping\n",
        "                    if not os.path.exists(SRC_PT_CANON):\n",
        "                        try:\n",
        "                            shutil.copyfile(p, SRC_PT_CANON)\n",
        "                            print(\"Copied candidate to canonical path:\", SRC_PT_CANON)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    export_pt_to_npz(p, npz_out=NPZ_OUT, overwrite=False)\n",
        "                    recovered = True\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(\"Failed loading candidate:\", p, \"->\", e)\n",
        "                continue\n",
        "        if not recovered:\n",
        "            print(\"‚ùå Could not recover a list-of-graphs .pt. Aborting NPZ creation.\")\n",
        "npz_files = sorted(glob.glob(os.path.join(NPZ_OUT, \"*.npz\")))\n",
        "print(\"Found NPZ count:\", len(npz_files))\n",
        "if len(npz_files) == 0:\n",
        "    raise SystemExit(\"No graphs (.npz) available. Provide a .pt or pre-exported .npz files in: \" + NPZ_OUT)\n",
        "\n",
        "# 2) Load demographics\n",
        "dem_path = find_demographics(DEM_PREF)\n",
        "if dem_path is None:\n",
        "    raise SystemExit(\"Demographics file not found. Upload your oasis_longitudinal_demographics* file to Drive or data/demographics/ and re-run.\")\n",
        "print(\"Using demographics file:\", dem_path)\n",
        "dem_map, dem_keys, id_col, label_col, dem_df = build_dem_map(dem_path)\n",
        "print(f\"Demographics loaded. id_col = {id_col}, label_col = {label_col}, dem entries = {len(dem_keys)}\")\n",
        "# quick sample\n",
        "print(\"Dem keys sample:\", dem_keys[:12])\n",
        "\n",
        "# 3) Perform strong direct mapping (exact match) and collect misses for fuzzy\n",
        "auto_matches = []\n",
        "miss_rows = []\n",
        "index_rows = []\n",
        "for p in npz_files:\n",
        "    sid_raw = extract_npz_sid(p)\n",
        "    sid_norm = str(sid_raw).strip() if sid_raw is not None else \"\"\n",
        "    label_val = None\n",
        "    chosen_key = None\n",
        "    # try exact OAS2_0001 style or numeric token matches\n",
        "    if sid_norm in dem_map:\n",
        "        chosen_key = sid_norm\n",
        "        label_val = dem_map[sid_norm]\n",
        "    else:\n",
        "        # try common padded numeric forms (e.g. '1' -> 'OAS2_0001')\n",
        "        digits = re.sub(r'\\D+', '', sid_norm or \"\")\n",
        "        if digits:\n",
        "            # attempt variants:\n",
        "            variants = [digits, digits.zfill(4), f\"OAS2_{digits.zfill(4)}\", f\"OAS_{digits.zfill(4)}\"]\n",
        "            for v in variants:\n",
        "                if v in dem_map:\n",
        "                    chosen_key = v; label_val = dem_map[v]; break\n",
        "    if chosen_key is not None:\n",
        "        # save labeled npz immediately\n",
        "        arr = np.load(p, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "        y = np.array([label_val])\n",
        "        outname = os.path.join(LABELED_OUT, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "        np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "        auto_matches.append({\"npz_file\":p, \"labeled_npz\": outname, \"npz_sid\": sid_norm, \"dem_key\": chosen_key, \"y\": label_val})\n",
        "        index_rows.append({\"npz_file\":p, \"npz_sid\":sid_norm, \"dem_key\": chosen_key, \"y\": label_val})\n",
        "    else:\n",
        "        miss_rows.append({\"npz_file\": p, \"npz_sid\": sid_norm})\n",
        "\n",
        "print(\"Strict auto-labeled count:\", len(auto_matches), \"Misses:\", len(miss_rows))\n",
        "\n",
        "# 4) Fuzzy-match suggestions for misses\n",
        "fuzzy_rows = []\n",
        "for m in miss_rows:\n",
        "    p = m['npz_file']; sid = str(m.get('npz_sid') or \"\")\n",
        "    cands = fuzzy_candidates_for_sid(sid, dem_keys, topk=6)\n",
        "    fuzzy_rows.append({\"npz_file\": p, \"npz_sid\": sid, \"candidates\": \";\".join([f\"{k}|{s:.3f}\" for k,s in cands])})\n",
        "# write suggestions csv\n",
        "fdf = pd.DataFrame(fuzzy_rows)\n",
        "fdf.to_csv(FUZZY_CSV, index=False)\n",
        "print(\"Saved fuzzy suggestions to:\", FUZZY_CSV)\n",
        "\n",
        "# 5) Provide interactive helpers (in-memory state + CSVs saved)\n",
        "# Build in-memory miss list with parsed candidates\n",
        "miss_list = []\n",
        "for idx, row in fdf.iterrows():\n",
        "    cand_text = row['candidates'] or \"\"\n",
        "    cands = []\n",
        "    if cand_text:\n",
        "        for token in cand_text.split(\";\"):\n",
        "            if \"|\" in token:\n",
        "                k, s = token.split(\"|\")\n",
        "                try: s = float(s)\n",
        "                except: s = 0.0\n",
        "                cands.append((k, s))\n",
        "    miss_list.append({\"idx\": int(idx), \"npz_file\": row['npz_file'], \"npz_sid\": row['npz_sid'], \"candidates\": cands, \"applied\": None})\n",
        "\n",
        "# keep track of applied decisions (updates CSVs when applied)\n",
        "applied_index_rows = list(index_rows)  # start with strict matches\n",
        "applied_misses = []\n",
        "\n",
        "# helper functions to expose to notebook user\n",
        "def show_miss(i):\n",
        "    \"\"\"Print details for miss index i (use index from CSV / printed table).\"\"\"\n",
        "    if i < 0 or i >= len(miss_list):\n",
        "        print(\"Index out of range:\", i); return\n",
        "    m = miss_list[i]\n",
        "    print(\"MISS INDEX:\", i)\n",
        "    print(\" NPZ file: \", m['npz_file'])\n",
        "    print(\" NPZ extracted sid:\", m['npz_sid'])\n",
        "    print(\" Candidates (dem_key | score):\")\n",
        "    for ci, (k,s) in enumerate(m['candidates']):\n",
        "        # show demographics label value too\n",
        "        demo_val = dem_map.get(k, None)\n",
        "        print(f\"  [{ci}] {k}  score={s:.3f}  -> label={demo_val}\")\n",
        "    # quick preview of npz structure\n",
        "    try:\n",
        "        arr = np.load(m['npz_file'], allow_pickle=True)\n",
        "        print(\" NPZ keys:\", list(arr.keys()))\n",
        "        x = arr.get('x', None)\n",
        "        if x is not None and getattr(x, 'shape', None) is not None:\n",
        "            print(\"  x.shape:\", np.asarray(x).shape, \" dtype:\", np.asarray(x).dtype)\n",
        "        pos = arr.get('pos', None)\n",
        "        if pos is not None and getattr(pos, 'shape', None) is not None:\n",
        "            print(\"  pos.shape:\", np.asarray(pos).shape)\n",
        "        ei = arr.get('edge_index', None)\n",
        "        if ei is not None:\n",
        "            try:\n",
        "                ei_a = np.asarray(ei)\n",
        "                print(\"  edge_index.shape:\", ei_a.shape)\n",
        "            except:\n",
        "                print(\"  edge_index: (could not parse shape)\")\n",
        "    except Exception as e:\n",
        "        print(\" Could not open npz:\", e)\n",
        "\n",
        "def review_and_apply(miss_idx, cand_idx):\n",
        "    \"\"\"Accept candidate cand_idx for miss miss_idx. Writes labeled npz and updates CSVs.\"\"\"\n",
        "    if miss_idx < 0 or miss_idx >= len(miss_list):\n",
        "        print(\"miss idx out of range\"); return\n",
        "    m = miss_list[miss_idx]\n",
        "    if cand_idx < 0 or cand_idx >= len(m['candidates']):\n",
        "        print(\"cand idx out of range\"); return\n",
        "    dem_key, score = m['candidates'][cand_idx]\n",
        "    label_val = dem_map.get(dem_key, None)\n",
        "    # load npz and save labeled npz\n",
        "    arr = np.load(m['npz_file'], allow_pickle=True)\n",
        "    x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([]))\n",
        "    ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "    y = np.array([label_val])\n",
        "    outname = os.path.join(LABELED_OUT, os.path.basename(m['npz_file']).replace(\".npz\",\"_labeled.npz\"))\n",
        "    np.savez_compressed(outname, x=x, pos=pos, edge_index=ei, y=y)\n",
        "    # record\n",
        "    applied_index_rows.append({\"npz_file\": m['npz_file'], \"npz_sid\": m['npz_sid'], \"dem_key\": dem_key, \"y\": label_val})\n",
        "    applied_misses.append({\"npz_file\": m['npz_file'], \"npz_sid\": m['npz_sid'], \"mapped_dem_key\": dem_key, \"y\": label_val, \"score\": score})\n",
        "    m['applied'] = (dem_key, score)\n",
        "    # persist small updates to CSVs\n",
        "    pd.DataFrame(applied_index_rows).to_csv(INDEX_CSV, index=False)\n",
        "    pd.DataFrame(applied_misses).to_csv(MISSES_CSV, index=False)\n",
        "    print(\"Applied mapping:\", os.path.basename(m['npz_file']), \"->\", dem_key, \" score=\", score)\n",
        "    print(\"Wrote labeled npz:\", outname)\n",
        "    print(\"Updated index CSV:\", INDEX_CSV, \"and misses CSV:\", MISSES_CSV)\n",
        "\n",
        "def auto_apply_threshold(threshold=0.90, dry_run=True):\n",
        "    \"\"\"Auto-accept candidates whose fuzzy score >= threshold. dry_run=True just prints what would be applied.\"\"\"\n",
        "    to_apply = []\n",
        "    for m in miss_list:\n",
        "        if m['applied'] is not None: continue\n",
        "        if not m['candidates']: continue\n",
        "        top = m['candidates'][0]\n",
        "        if top[1] >= threshold:\n",
        "            to_apply.append((m, top))\n",
        "    print(f\"Found {len(to_apply)} auto-apply candidates with score >= {threshold} (dry_run={dry_run})\")\n",
        "    if dry_run:\n",
        "        for m,top in to_apply[:50]:\n",
        "            print(\" WOULD APPLY:\", os.path.basename(m['npz_file']), \"->\", top[0], \"score=\", top[1])\n",
        "        return to_apply\n",
        "    else:\n",
        "        for m,top in to_apply:\n",
        "            idx = next((i for i,mm in enumerate(miss_list) if mm['npz_file']==m['npz_file']), None)\n",
        "            if idx is not None:\n",
        "                ci = 0  # top candidate\n",
        "                review_and_apply(idx, ci)\n",
        "        return len(to_apply)\n",
        "\n",
        "def build_final_pt(overwrite=True):\n",
        "    \"\"\"Aggregate labeled npz files in LABELED_OUT into a single .pt (torch_geometric.Data list).\"\"\"\n",
        "    npz_labeled = sorted(glob.glob(os.path.join(LABELED_OUT, \"*_labeled.npz\")))\n",
        "    if not npz_labeled:\n",
        "        print(\"No labeled npz files found in\", LABELED_OUT)\n",
        "        return None\n",
        "    data_list = []\n",
        "    for p in npz_labeled:\n",
        "        arr = np.load(p, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([]))\n",
        "        pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "        y = arr.get('y', np.array([]))\n",
        "        # convert to torch geometric Data where possible\n",
        "        if torch is not None:\n",
        "            try:\n",
        "                xt = torch.tensor(x, dtype=torch.float) if getattr(x,'size', None) and np.asarray(x).size else None\n",
        "                post = torch.tensor(pos, dtype=torch.float) if getattr(pos,'size', None) and np.asarray(pos).size else None\n",
        "                eit = torch.tensor(ei, dtype=torch.long) if getattr(ei,'size', None) and np.asarray(ei).size else None\n",
        "                yt = torch.tensor(y, dtype=torch.float) if getattr(y,'size', None) and np.asarray(y).size else None\n",
        "                g = Data(x=xt, edge_index=eit, pos=post, y=yt)\n",
        "                # store a subject id attribute for traceability\n",
        "                g.subject_id = os.path.basename(p).replace(\"_labeled.npz\",\"\")\n",
        "                data_list.append(g)\n",
        "            except Exception:\n",
        "                # fallback: store a dict-like object\n",
        "                data_list.append({\"x\": x, \"edge_index\": ei, \"pos\": pos, \"y\": y, \"subject_id\": os.path.basename(p)})\n",
        "        else:\n",
        "            data_list.append({\"x\": x, \"edge_index\": ei, \"pos\": pos, \"y\": y, \"subject_id\": os.path.basename(p)})\n",
        "    # save\n",
        "    if os.path.exists(FINAL_PT) and not overwrite:\n",
        "        print(\"Final .pt exists and overwrite=False:\", FINAL_PT)\n",
        "    else:\n",
        "        if torch is not None and isinstance(data_list[0], Data):\n",
        "            torch.save(data_list, FINAL_PT)\n",
        "            print(\"Saved final labeled .pt (list-of-Data) to:\", FINAL_PT)\n",
        "        else:\n",
        "            # save generic pickled list\n",
        "            import pickle\n",
        "            with open(FINAL_PT, \"wb\") as f:\n",
        "                pickle.dump(data_list, f)\n",
        "            print(\"Saved final labeled pickled list to:\", FINAL_PT)\n",
        "    return FINAL_PT\n",
        "\n",
        "# Save initial index/miss CSVs (pre-application)\n",
        "pd.DataFrame(index_rows).to_csv(INDEX_CSV, index=False)\n",
        "pd.DataFrame([{\"npz_file\":m[\"npz_file\"], \"npz_sid\":m[\"npz_sid\"], \"candidates\": \";\".join([f\"{k}|{s:.3f}\" for k,s in m['candidates']])} for m in miss_list]).to_csv(MISSES_CSV, index=False)\n",
        "\n",
        "print(\"Pipeline complete.\")\n",
        "print(\"Counts: strict_labeled =\", len(auto_matches), \"misses_with_suggestions =\", len(miss_list))\n",
        "print(\"Fuzzy suggestions saved to:\", FUZZY_CSV)\n",
        "print(\"Index CSV (initial):\", INDEX_CSV)\n",
        "print(\"Miss CSV (initial):\", MISSES_CSV)\n",
        "print(\"\")\n",
        "print(\"Usage (examples):\")\n",
        "print(\"  show_miss(0)           # inspect miss 0\")\n",
        "print(\"  review_and_apply(0, 0) # accept candidate 0 for miss 0\")\n",
        "print(\"  auto_apply_threshold(0.90, dry_run=True)  # preview\")\n",
        "print(\"  auto_apply_threshold(0.90, dry_run=False) # apply\")\n",
        "print(\"  build_final_pt(overwrite=True)            # aggregate labeled npz -> final .pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyMVrJTbuW7w",
        "outputId": "b809e3d5-3b34-435e-9cad-3a4e62c337b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING pipeline. Base: /content/drive/MyDrive/oasis_project\n",
            "Found NPZ count: 209\n",
            "Using demographics file: /content/drive/MyDrive/oasis_project/data/demographics/oasis2_demographics.xlsx\n",
            "Demographics loaded. id_col = Subject ID, label_col = CDR, dem entries = 150\n",
            "Dem keys sample: ['OAS2_0001', 'OAS2_0002', 'OAS2_0004', 'OAS2_0005', 'OAS2_0007', 'OAS2_0008', 'OAS2_0009', 'OAS2_0010', 'OAS2_0012', 'OAS2_0013', 'OAS2_0014', 'OAS2_0016']\n",
            "Strict auto-labeled count: 150 Misses: 59\n",
            "Saved fuzzy suggestions to: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "Pipeline complete.\n",
            "Counts: strict_labeled = 150 misses_with_suggestions = 59\n",
            "Fuzzy suggestions saved to: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "Index CSV (initial): /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv\n",
            "Miss CSV (initial): /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n",
            "\n",
            "Usage (examples):\n",
            "  show_miss(0)           # inspect miss 0\n",
            "  review_and_apply(0, 0) # accept candidate 0 for miss 0\n",
            "  auto_apply_threshold(0.90, dry_run=True)  # preview\n",
            "  auto_apply_threshold(0.90, dry_run=False) # apply\n",
            "  build_final_pt(overwrite=True)            # aggregate labeled npz -> final .pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_apply_threshold(0.90, dry_run=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpgb9jn0uxsS",
        "outputId": "abac748a-5cc0-4fc9-99f5-30d0b8e9239c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 auto-apply candidates with score >= 0.9 (dry_run=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_apply_threshold(0.90, dry_run=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN4Mak7BwU6L",
        "outputId": "6861b887-37eb-489e-bbe3-d8a741783162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 auto-apply candidates with score >= 0.9 (dry_run=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_miss(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-8xsN5-wXYc",
        "outputId": "75d09ec1-a26a-42d0-b89d-a65b4669f722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISS INDEX: 3\n",
            " NPZ file:  /content/drive/MyDrive/oasis_project/data/graphs/npz_graphs_resave/graph_0011_subj_11.npz\n",
            " NPZ extracted sid: 0011\n",
            " Candidates (dem_key | score):\n",
            "  [0] OAS2_0001  score=0.462  -> label=0.0\n",
            "  [1] OAS2_0010  score=0.462  -> label=0.5\n",
            "  [2] OAS2_0012  score=0.462  -> label=0.0\n",
            "  [3] OAS2_0013  score=0.462  -> label=0.0\n",
            "  [4] OAS2_0014  score=0.462  -> label=1.0\n",
            "  [5] OAS2_0016  score=0.462  -> label=0.5\n",
            " NPZ keys: ['x', 'pos', 'edge_index']\n",
            "  x.shape: (1575, 50)  dtype: float32\n",
            "  pos.shape: (0,)\n",
            "  edge_index.shape: (2, 22062)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_and_apply(miss_idx=3, cand_idx=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6n5D-dowcs_",
        "outputId": "53a38681-e075-427d-aeae-ebfcd1c3dc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied mapping: graph_0011_subj_11.npz -> OAS2_0001  score= 0.462\n",
            "Wrote labeled npz: /content/drive/MyDrive/oasis_project/data/graphs/labeled_npz_auto/graph_0011_subj_11_labeled.npz\n",
            "Updated index CSV: /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv and misses CSV: /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "build_final_pt(overwrite=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FMT-vPkkwe15",
        "outputId": "285a35e6-d96b-4646-abee-0342d4f87382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved final labeled .pt (list-of-Data) to: /content/drive/MyDrive/oasis_project/data/graphs/oasis2_graphs_labeled_auto_final.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/oasis_project/data/graphs/oasis2_graphs_labeled_auto_final.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE-CELL: Aggressive / fuzzy + manual-apply relabeler for 04 -> attempt FULL coverage\n",
        "# Paste & run in 04_label_map_and_checks.ipynb (Colab). Installs rapidfuzz if missing.\n",
        "import os, glob, re, numpy as np, pandas as pd, shutil, math\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "GRAPHS_DIR = os.path.join(BASE, \"data\", \"graphs\")\n",
        "NPZ_FOLDER = os.path.join(GRAPHS_DIR, \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(GRAPHS_DIR, \"labeled_npz_full\")\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "OUT_INDEX = os.path.join(GRAPHS_DIR, \"final_graph_label_index_auto.csv\")\n",
        "OUT_MISSES = os.path.join(GRAPHS_DIR, \"final_label_misses_auto.csv\")\n",
        "OUT_SUGGEST = os.path.join(GRAPHS_DIR, \"label_misses_fuzzy_suggestions_rf.csv\")\n",
        "FINAL_PT = os.path.join(GRAPHS_DIR, \"oasis2_graphs_labeled_auto_full.pt\")\n",
        "MANUAL_MAP_CSV = os.path.join(GRAPHS_DIR, \"manual_mappings_to_apply.csv\")  # optional - you can edit this and re-run\n",
        "\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "# install rapidfuzz if not present (fast fuzzy)\n",
        "try:\n",
        "    from rapidfuzz import process, fuzz\n",
        "except Exception:\n",
        "    print(\"Installing rapidfuzz...\")\n",
        "    !pip -q install rapidfuzz\n",
        "    from rapidfuzz import process, fuzz\n",
        "\n",
        "# helpers\n",
        "def safe_load_npz(p):\n",
        "    try:\n",
        "        return np.load(p, allow_pickle=True)\n",
        "    except Exception as e:\n",
        "        print(\"Failed np.load:\", p, e); return {}\n",
        "\n",
        "def normalize_dem_key(k):\n",
        "    if k is None: return None\n",
        "    k = str(k).strip()\n",
        "    return k\n",
        "\n",
        "def generate_variants(key):\n",
        "    \"\"\"Return a set of string variants for a dem key to maximize matching chance.\"\"\"\n",
        "    if key is None: return set()\n",
        "    s = str(key).strip()\n",
        "    variants = set()\n",
        "    variants.add(s)\n",
        "    variants.add(s.upper())\n",
        "    variants.add(s.lower())\n",
        "    # strip prefixes/suffixes and common separators\n",
        "    variants.add(re.sub(r'[^0-9A-Za-z]', '', s))\n",
        "    # zero-pad numeric part if found\n",
        "    m = re.search(r'(\\d+)', s)\n",
        "    if m:\n",
        "        num = m.group(1)\n",
        "        for z in (0,1,2,3,4):\n",
        "            variants.add(num.zfill(len(num)+z))\n",
        "        # prefix forms\n",
        "        variants.add(\"OAS2_\" + num.zfill(4))\n",
        "        variants.add(\"OAS2\" + num.zfill(4))\n",
        "        variants.add(\"OAS2-\" + num.zfill(4))\n",
        "        variants.add(num)\n",
        "    # drop leading non-digits\n",
        "    variants.add(re.sub(r'^[^\\d]+', '', s))\n",
        "    # letters-only, digits-only\n",
        "    variants.add(re.sub(r'\\d+', '', s))\n",
        "    variants.add(''.join(ch for ch in s if ch.isalnum()))\n",
        "    # small-case digits-only\n",
        "    variants = set(v for v in variants if v is not None and len(str(v))>0)\n",
        "    return set(str(v) for v in variants)\n",
        "\n",
        "# Load demographics\n",
        "if not os.path.exists(DEM_PATH):\n",
        "    raise SystemExit(f\"Demographics file not found at {DEM_PATH}. Place it there or update DEM_PATH and re-run.\")\n",
        "print(\"Loading demographics:\", DEM_PATH)\n",
        "if DEM_PATH.lower().endswith('.csv'):\n",
        "    dem = pd.read_csv(DEM_PATH)\n",
        "else:\n",
        "    dem = pd.read_excel(DEM_PATH)\n",
        "dem.rename(columns={c:c.strip() for c in dem.columns}, inplace=True)\n",
        "# detect columns\n",
        "cols = list(dem.columns)\n",
        "id_col = next((c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), cols[0])\n",
        "label_col = next((c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), cols[-1])\n",
        "print(\"Detected id_col:\", id_col, \"label_col:\", label_col)\n",
        "# build mapping with many variants\n",
        "dem_map = {}\n",
        "for _, row in dem.iterrows():\n",
        "    raw_key = row.get(id_col)\n",
        "    val = row.get(label_col)\n",
        "    if pd.isna(raw_key) or pd.isna(val): continue\n",
        "    key = normalize_dem_key(raw_key)\n",
        "    for v in generate_variants(key):\n",
        "        dem_map[str(v)] = val\n",
        "# also create a reverse list of canonical keys for fuzzy candidates\n",
        "dem_keys = sorted(set(normalize_dem_key(k) for k in dem[id_col].astype(str).tolist() if not pd.isna(k)))\n",
        "print(\"Demographic entries (canonical):\", len(dem_keys))\n",
        "\n",
        "# discover npz files\n",
        "npz_files = sorted(glob.glob(os.path.join(NPZ_FOLDER, \"*.npz\")))\n",
        "if len(npz_files)==0:\n",
        "    raise SystemExit(f\"No npz files found in {NPZ_FOLDER}\")\n",
        "print(\"Found NPZ count:\", len(npz_files))\n",
        "\n",
        "# function to extract candidate ids from filename and inside file\n",
        "def extract_candidates_from_npz(p):\n",
        "    fname = os.path.basename(p)\n",
        "    sids = set()\n",
        "    # common patterns: OAS2_0123, subj_12, sub_12, 12\n",
        "    for m in re.finditer(r'(OAS2[_\\-]?\\d{1,6})', fname, flags=re.IGNORECASE):\n",
        "        sids.add(m.group(0))\n",
        "    for m in re.finditer(r'(\\d{1,6})', fname):\n",
        "        sids.add(m.group(0))\n",
        "    for m in re.finditer(r'(subj[_\\-]?\\d+|sub[_\\-]?\\d+|subj\\d+|sub\\d+)', fname, flags=re.IGNORECASE):\n",
        "        sids.add(m.group(0))\n",
        "    # try to read subject inside npz if present\n",
        "    try:\n",
        "        arr = safe_load_npz(p)\n",
        "        # keys often 'subject_id' or 'sid' or 'filename'\n",
        "        for k in ('subject_id','sid','id','filename','name'):\n",
        "            if k in arr:\n",
        "                v = arr[k]\n",
        "                if isinstance(v, (np.ndarray, list)) and len(np.asarray(v))>0:\n",
        "                    v = np.asarray(v).tolist()\n",
        "                sids.add(str(v))\n",
        "        # also some NPZs store arrays with dtype object in 'pos' that include label ‚Äì ignore generally\n",
        "    except Exception:\n",
        "        pass\n",
        "    # normalize\n",
        "    sids = set(str(x).strip() for x in sids if x is not None and str(x).strip()!='')\n",
        "    return sids\n",
        "\n",
        "# cascade matching\n",
        "auto_matches = []\n",
        "misses = []\n",
        "fuzzy_suggestions = []\n",
        "\n",
        "# fuzzy helper using rapidfuzz\n",
        "def fuzzy_best(query, choices, limit=5):\n",
        "    if not query or len(choices)==0: return []\n",
        "    res = process.extract(query, choices, scorer=fuzz.WRatio, limit=limit)\n",
        "    # res = list of (candidate, score, idx)\n",
        "    return [(r[0], float(r[1])) for r in res]\n",
        "\n",
        "# main loop\n",
        "for p in tqdm(npz_files, desc=\"Auto-matching NPZs\"):\n",
        "    arr = safe_load_npz(p)\n",
        "    fname = os.path.basename(p)\n",
        "    # extract x if present - only for stats\n",
        "    try:\n",
        "        x = arr.get('x', None)\n",
        "        n_nodes = int(np.asarray(x).shape[0]) if x is not None else 0\n",
        "        feat_dim = int(np.asarray(x).shape[1]) if (x is not None and np.asarray(x).ndim>1) else 1\n",
        "    except Exception:\n",
        "        n_nodes=0; feat_dim=0\n",
        "    candidates = extract_candidates_from_npz(p)\n",
        "    matched = False\n",
        "    chosen_key = None\n",
        "    chosen_val = None\n",
        "\n",
        "    # Strategy 1: direct exact match using many variants\n",
        "    for c in list(candidates):\n",
        "        for variant in generate_variants(c):\n",
        "            if variant in dem_map:\n",
        "                chosen_key = variant; chosen_val = dem_map[variant]; matched=True; break\n",
        "        if matched: break\n",
        "\n",
        "    # Strategy 2: digits-only exact match\n",
        "    if not matched:\n",
        "        for c in list(candidates):\n",
        "            digits = re.sub(r'\\D+','', str(c))\n",
        "            if digits and digits in dem_map:\n",
        "                chosen_key=digits; chosen_val=dem_map[digits]; matched=True; break\n",
        "\n",
        "    # Strategy 3: try concatenating prefix forms (OAS2_...) if dem keys look like that\n",
        "    if not matched:\n",
        "        for c in list(candidates):\n",
        "            digits = re.sub(r'\\D+','', str(c))\n",
        "            if digits:\n",
        "                trial = f\"OAS2_{digits.zfill(4)}\"\n",
        "                if trial in dem_map:\n",
        "                    chosen_key=trial; chosen_val=dem_map[trial]; matched=True; break\n",
        "\n",
        "    # Strategy 4: try exact match on filename base (strip non-alnum)\n",
        "    if not matched:\n",
        "        base = re.sub(r'[^0-9A-Za-z]', '', os.path.splitext(fname)[0])\n",
        "        if base in dem_map:\n",
        "            chosen_key=base; chosen_val=dem_map[base]; matched=True\n",
        "\n",
        "    # Strategy 5: fuzzy match using rapidfuzz between extracted candidate tokens and canonical dem_keys\n",
        "    fuzzy_candidates = []\n",
        "    if not matched:\n",
        "        # build queries: tokens + filename base\n",
        "        queries = list(candidates) + [os.path.splitext(fname)[0], re.sub(r'[^0-9A-Za-z]','',os.path.splitext(fname)[0])]\n",
        "        # dedupe\n",
        "        queries = [q for q in dict.fromkeys(queries) if q]\n",
        "        all_matches = []\n",
        "        for q in queries:\n",
        "            out = fuzzy_best(q, dem_keys, limit=5)\n",
        "            for cand,score in out:\n",
        "                all_matches.append((q,cand,score))\n",
        "        # reduce to best candidate by score\n",
        "        if all_matches:\n",
        "            best = sorted(all_matches, key=lambda x: x[2], reverse=True)[0]\n",
        "            q,cand,score = best\n",
        "            # apply high confidence threshold (>=90)\n",
        "            if score >= 90:\n",
        "                chosen_key = cand; chosen_val = dem.loc[dem[id_col].astype(str).str.strip()==cand, label_col].iloc[0] if (dem[id_col].astype(str).str.strip()==cand).any() else dem_map.get(cand)\n",
        "                matched=True\n",
        "            # otherwise record suggestions\n",
        "            else:\n",
        "                # prepare top suggestions string\n",
        "                grouped = {}\n",
        "                for q0,cand0,score0 in sorted(all_matches, key=lambda x:-x[2])[:10]:\n",
        "                    grouped[cand0] = max(grouped.get(cand0, 0), score0)\n",
        "                candstr = \";\".join(f\"{k}|{v:.3f}\" for k,v in sorted(grouped.items(), key=lambda x:-x[1])[:10])\n",
        "                fuzzy_suggestions.append({\"npz_file\":p, \"npz_sid\": \";\".join(sorted(candidates)) if candidates else \"\", \"candidates\": candstr})\n",
        "    # done strategies\n",
        "\n",
        "    if matched and chosen_key is not None:\n",
        "        # write labeled npz\n",
        "        y = np.array([float(chosen_val)]) if not pd.isna(chosen_val) else np.array([str(chosen_val)])\n",
        "        outp = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "        pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0), dtype=np.int64))\n",
        "        xdata = arr.get('x', np.array([]))\n",
        "        np.savez_compressed(outp, x=xdata, pos=pos, edge_index=ei, y=y)\n",
        "        auto_matches.append({\"npz_file\":p, \"npz_sid\": \";\".join(sorted(candidates)) if candidates else \"\", \"dem_key\": chosen_key, \"y\": chosen_val, \"labeled_npz\": outp, \"n_nodes\": n_nodes, \"feat_dim\": feat_dim})\n",
        "    else:\n",
        "        misses.append({\"npz_file\":p, \"npz_sid\": \";\".join(sorted(candidates)) if candidates else \"\"})\n",
        "\n",
        "# If we created fuzzy_suggestions, merge with misses list entries\n",
        "# ensure there is one suggestion row per miss\n",
        "fuzzy_df = pd.DataFrame(fuzzy_suggestions)\n",
        "miss_df = pd.DataFrame(misses)\n",
        "if not miss_df.empty:\n",
        "    if not fuzzy_df.empty:\n",
        "        # join on npz_file\n",
        "        merged = miss_df.merge(fuzzy_df, on=\"npz_file\", how=\"left\")\n",
        "    else:\n",
        "        merged = miss_df.copy()\n",
        "        merged[\"candidates\"] = \"\"\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=[\"npz_file\",\"npz_sid\",\"candidates\"])\n",
        "\n",
        "# Save intermediate CSVs\n",
        "index_rows = auto_matches.copy()\n",
        "pd.DataFrame(index_rows).to_csv(OUT_INDEX, index=False)\n",
        "merged.to_csv(OUT_MISSES, index=False)\n",
        "pd.DataFrame(fuzzy_suggestions).to_csv(OUT_SUGGEST, index=False)\n",
        "print(\"Auto-labeled:\", len(auto_matches), \"Misses:\", len(miss_df))\n",
        "print(\"Saved index:\", OUT_INDEX)\n",
        "print(\"Saved misses:\", OUT_MISSES)\n",
        "print(\"Saved fuzzy suggestions:\", OUT_SUGGEST)\n",
        "\n",
        "# APPLY manual_mappings.csv if present (columns: npz_file,mapped_dem_key) - user can create/edit this CSV and re-run cell\n",
        "if os.path.exists(MANUAL_MAP_CSV):\n",
        "    print(\"Found manual mappings CSV - applying manual mappings from:\", MANUAL_MAP_CSV)\n",
        "    mm = pd.read_csv(MANUAL_MAP_CSV)\n",
        "    applied = 0\n",
        "    for _, r in mm.iterrows():\n",
        "        p = r.get(\"npz_file\")\n",
        "        map_key = r.get(\"mapped_dem_key\") or r.get(\"mapped_key\") or r.get(\"dem_key\")\n",
        "        if pd.isna(p) or pd.isna(map_key): continue\n",
        "        p = str(p).strip()\n",
        "        map_key = str(map_key).strip()\n",
        "        if not os.path.exists(p):\n",
        "            print(\"Manual mapping: npz file not found:\", p); continue\n",
        "        # find canonical dem value\n",
        "        demval = None\n",
        "        # try exact match on id_col in dem\n",
        "        row_sel = dem[dem[id_col].astype(str).str.strip()==map_key]\n",
        "        if not row_sel.empty:\n",
        "            demval = float(row_sel[label_col].iloc[0])\n",
        "        else:\n",
        "            # try dem_map lookup for variants\n",
        "            for v in generate_variants(map_key):\n",
        "                if v in dem_map:\n",
        "                    demval = float(dem_map[v]); break\n",
        "        if demval is None:\n",
        "            print(\"Mapped dem key not found in demographics:\", map_key); continue\n",
        "        # write labeled npz\n",
        "        arr = safe_load_npz(p)\n",
        "        y = np.array([demval])\n",
        "        outp = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "        np.savez_compressed(outp, x=arr.get('x', np.array([])), pos=arr.get('pos', np.array([])), edge_index=arr.get('edge_index', np.empty((2,0),dtype=np.int64)), y=y)\n",
        "        applied += 1\n",
        "    print(\"Applied manual mappings:\", applied)\n",
        "\n",
        "# After auto + manual, rebuild final index and misses and create final .pt\n",
        "# collect all labeled npz in LABELED_DIR\n",
        "labeled_npzs = sorted(glob.glob(os.path.join(LABELED_DIR, \"*_labeled.npz\")))\n",
        "print(\"Found labeled npzs (after manual apply):\", len(labeled_npzs))\n",
        "\n",
        "# build final index rows\n",
        "final_index = []\n",
        "for lp in labeled_npzs:\n",
        "    arr = safe_load_npz(lp)\n",
        "    fname = os.path.basename(lp)\n",
        "    # try to recover original npz_file by searching name prefix inside NPZ_FOLDER\n",
        "    original_candidates = sorted(glob.glob(os.path.join(NPZ_FOLDER, fname.replace(\"_labeled.npz\",\"*.npz\"))))\n",
        "    orig = original_candidates[0] if original_candidates else \"\"\n",
        "    y = arr.get('y', None)\n",
        "    yv = float(np.asarray(y).reshape(-1)[0]) if y is not None and np.asarray(y).size>0 else None\n",
        "    x = arr.get('x', None)\n",
        "    n_nodes = int(np.asarray(x).shape[0]) if x is not None else 0\n",
        "    feat_dim = int(np.asarray(x).shape[1]) if (x is not None and np.asarray(x).ndim>1) else 1\n",
        "    final_index.append({\"npz_file\": orig, \"labeled_npz\": lp, \"n_nodes\": n_nodes, \"feat_dim\": feat_dim, \"y\": yv})\n",
        "\n",
        "# recompute misses (npz files without labeled counterpart)\n",
        "all_npz_set = set(npz_files)\n",
        "labeled_orig_set = set(r[\"npz_file\"] for r in final_index if r[\"npz_file\"])\n",
        "remaining_npzs = sorted(list(all_npz_set - labeled_orig_set))\n",
        "final_misses_rows = []\n",
        "for p in remaining_npzs:\n",
        "    final_misses_rows.append({\"npz_file\": p, \"npz_sid\": \";\".join(extract_candidates_from_npz(p))})\n",
        "\n",
        "pd.DataFrame(final_index).to_csv(OUT_INDEX, index=False)\n",
        "pd.DataFrame(final_misses_rows).to_csv(OUT_MISSES, index=False)\n",
        "print(\"Final index saved:\", OUT_INDEX)\n",
        "print(\"Final misses saved:\", OUT_MISSES, \"count:\", len(final_misses_rows))\n",
        "\n",
        "# build final .pt (PyG Data objects) from labeled_npzs\n",
        "try:\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "    data_list = []\n",
        "    for rec in final_index:\n",
        "        lp = rec[\"labeled_npz\"]\n",
        "        if not lp or not os.path.exists(lp): continue\n",
        "        arr = np.load(lp, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([]))\n",
        "        pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', np.empty((2,0),dtype=np.int64))\n",
        "        y = arr.get('y', np.array([]))\n",
        "        # convert to tensors where possible\n",
        "        try:\n",
        "            xt = torch.tensor(x, dtype=torch.float) if np.asarray(x).size else None\n",
        "        except Exception:\n",
        "            xt = None\n",
        "        try:\n",
        "            post = torch.tensor(pos, dtype=torch.float) if np.asarray(pos).size else None\n",
        "        except Exception:\n",
        "            post = None\n",
        "        try:\n",
        "            eit = torch.tensor(ei, dtype=torch.long) if np.asarray(ei).size else None\n",
        "        except Exception:\n",
        "            eit = None\n",
        "        try:\n",
        "            yt = torch.tensor(np.asarray(y).reshape(-1), dtype=torch.float) if np.asarray(y).size else None\n",
        "        except Exception:\n",
        "            yt = None\n",
        "        # create Data (best-effort)\n",
        "        try:\n",
        "            g = Data(x=xt, edge_index=eit, pos=post, y=yt)\n",
        "            # attach subject id guessed from filename\n",
        "            g.subject_id = os.path.basename(lp).replace(\"_labeled.npz\",\"\")\n",
        "            data_list.append(g)\n",
        "        except Exception:\n",
        "            continue\n",
        "    if data_list:\n",
        "        torch.save(data_list, FINAL_PT)\n",
        "        print(\"Saved final labeled .pt with\", len(data_list), \"graphs to:\", FINAL_PT)\n",
        "    else:\n",
        "        print(\"No Data objects created; final .pt not saved.\")\n",
        "except Exception as e:\n",
        "    print(\"Could not build final .pt:\", e)\n",
        "\n",
        "print(\"DONE. Summary:\")\n",
        "print(\"  Auto-labeled (initial):\", len(auto_matches))\n",
        "print(\"  Labeled npzs (final):\", len(labeled_npzs))\n",
        "print(\"  Remaining misses:\", len(final_misses_rows))\n",
        "print(\"Files to inspect / edit manually:\")\n",
        "print(\"  - Fuzzy suggestions:\", OUT_SUGGEST)\n",
        "print(\"  - Final misses CSV (edit to provide mapped_dem_key or use manual_mappings_to_apply.csv):\", OUT_MISSES)\n",
        "print(\"  - To manually supply mappings, create/edit:\", MANUAL_MAP_CSV, \"with columns: npz_file,mapped_dem_key and re-run this cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5siW8Q2wg1A",
        "outputId": "a23811e2-6012-4dbd-f812-90cabf45ed1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading demographics: /content/drive/MyDrive/oasis_project/data/demographics/oasis2_demographics.xlsx\n",
            "Detected id_col: Subject ID label_col: CDR\n",
            "Demographic entries (canonical): 150\n",
            "Found NPZ count: 209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Auto-matching NPZs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 209/209 [00:11<00:00, 17.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-labeled: 163 Misses: 46\n",
            "Saved index: /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv\n",
            "Saved misses: /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n",
            "Saved fuzzy suggestions: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "Found labeled npzs (after manual apply): 163\n",
            "Final index saved: /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv\n",
            "Final misses saved: /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv count: 46\n",
            "Saved final labeled .pt with 163 graphs to: /content/drive/MyDrive/oasis_project/data/graphs/oasis2_graphs_labeled_auto_full.pt\n",
            "DONE. Summary:\n",
            "  Auto-labeled (initial): 163\n",
            "  Labeled npzs (final): 163\n",
            "  Remaining misses: 46\n",
            "Files to inspect / edit manually:\n",
            "  - Fuzzy suggestions: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "  - Final misses CSV (edit to provide mapped_dem_key or use manual_mappings_to_apply.csv): /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv\n",
            "  - To manually supply mappings, create/edit: /content/drive/MyDrive/oasis_project/data/graphs/manual_mappings_to_apply.csv with columns: npz_file,mapped_dem_key and re-run this cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE-CELL: Auto-accept top fuzzy suggestions above threshold and rebuild final .pt (run in 04)\n",
        "import os, glob, pandas as pd, numpy as np, re, shutil\n",
        "from pathlib import Path\n",
        "BASE = \"/content/drive/MyDrive/oasis_project\"\n",
        "GRAPHS_DIR = os.path.join(BASE, \"data\", \"graphs\")\n",
        "NPZ_FOLDER = os.path.join(GRAPHS_DIR, \"npz_graphs_resave\")\n",
        "LABELED_DIR = os.path.join(GRAPHS_DIR, \"labeled_npz_full\")   # same dir used earlier\n",
        "FUZZY_CSV = os.path.join(GRAPHS_DIR, \"label_misses_fuzzy_suggestions_rf.csv\")\n",
        "FINAL_INDEX = os.path.join(GRAPHS_DIR, \"final_graph_label_index_auto.csv\")\n",
        "FINAL_MISSES = os.path.join(GRAPHS_DIR, \"final_label_misses_auto.csv\")\n",
        "FINAL_PT = os.path.join(GRAPHS_DIR, \"oasis2_graphs_labeled_auto_full.pt\")\n",
        "DEM_PATH = os.path.join(BASE, \"data\", \"demographics\", \"oasis2_demographics.xlsx\")\n",
        "THRESH = 0.90   # adjust: lower to accept more fuzzy matches (e.g. 0.80)\n",
        "\n",
        "os.makedirs(LABELED_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(FUZZY_CSV):\n",
        "    raise SystemExit(f\"Fuzzy suggestions CSV not found: {FUZZY_CSV}\")\n",
        "\n",
        "print(\"Reading fuzzy suggestions:\", FUZZY_CSV)\n",
        "df = pd.read_csv(FUZZY_CSV)\n",
        "# expected 'npz_file' and 'candidates' columns; candidates formatted like \"OAS2_0111|0.900;OAS2_0112|0.900;...\"\n",
        "applied = []\n",
        "skipped = []\n",
        "# load demographics to map candidate -> value\n",
        "if DEM_PATH.lower().endswith('.csv'):\n",
        "    dem = pd.read_csv(DEM_PATH)\n",
        "else:\n",
        "    dem = pd.read_excel(DEM_PATH)\n",
        "dem.rename(columns={c:c.strip() for c in dem.columns}, inplace=True)\n",
        "cols = list(dem.columns)\n",
        "id_col = next((c for c in cols if any(tok in c.upper() for tok in (\"MRI\",\"MRI_ID\",\"MRIID\",\"SUBJ\",\"SUBJECT\",\"ID\"))), cols[0])\n",
        "label_col = next((c for c in cols if any(tok in c.upper() for tok in (\"CDR\",\"CDR_GLOBAL\",\"DEMENTIA\",\"DIAG\",\"SEVERITY\"))), cols[-1])\n",
        "\n",
        "def dem_value_for_key(k):\n",
        "    # k expected like 'OAS2_0111' or '0111' etc.\n",
        "    k = str(k).strip()\n",
        "    # exact\n",
        "    sel = dem[dem[id_col].astype(str).str.strip()==k]\n",
        "    if not sel.empty:\n",
        "        return float(sel[label_col].iloc[0])\n",
        "    # try digits-only\n",
        "    digits = re.sub(r'\\D+','',k)\n",
        "    if digits:\n",
        "        sel = dem[dem[id_col].astype(str).str.contains(digits, na=False)]\n",
        "        if not sel.empty:\n",
        "            return float(sel[label_col].iloc[0])\n",
        "    return None\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    p = row.get('npz_file')\n",
        "    candstr = row.get('candidates', \"\")\n",
        "    if pd.isna(p) or not p: continue\n",
        "    # parse top candidate\n",
        "    parts = [s for s in str(candstr).split(';') if s]\n",
        "    if not parts:\n",
        "        skipped.append(p); continue\n",
        "    top = parts[0]  # \"OAS2_0111|0.900\"\n",
        "    if '|' in top:\n",
        "        k, s = top.split('|',1)\n",
        "        score = float(s)\n",
        "    else:\n",
        "        k = top; score = 0.0\n",
        "    if score >= THRESH:\n",
        "        val = dem_value_for_key(k)\n",
        "        if val is None:\n",
        "            # try to strip leading non-digits and lookup\n",
        "            k2 = re.sub(r'^[^\\d]+','',k)\n",
        "            val = dem_value_for_key(k2)\n",
        "        if val is None:\n",
        "            skipped.append(p)\n",
        "            continue\n",
        "        # load original npz, write labeled npz\n",
        "        try:\n",
        "            arr = np.load(p, allow_pickle=True)\n",
        "        except Exception as e:\n",
        "            print(\"Failed load npz:\", p, e); skipped.append(p); continue\n",
        "        x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([]))\n",
        "        ei = arr.get('edge_index', arr.get('edges', np.empty((2,0), dtype=np.int64)))\n",
        "        y = np.array([float(val)])\n",
        "        outp = os.path.join(LABELED_DIR, os.path.basename(p).replace(\".npz\",\"_labeled.npz\"))\n",
        "        np.savez_compressed(outp, x=x if x is not None else np.array([]), pos=pos if pos is not None else np.array([]),\n",
        "                            edge_index=ei if ei is not None else np.empty((2,0), dtype=np.int64), y=y)\n",
        "        applied.append({\"npz_file\":p, \"mapped_key\":k, \"score\":score, \"y\":val, \"labeled_npz\":outp})\n",
        "    else:\n",
        "        skipped.append(p)\n",
        "\n",
        "print(\"Auto-applied matches above threshold:\", len(applied))\n",
        "print(\"Skipped (below threshold or failed):\", len(skipped))\n",
        "\n",
        "# Rebuild index + misses and .pt (same logic as prior cell)\n",
        "# gather all labeled npzs\n",
        "labeled_npzs = sorted(glob.glob(os.path.join(LABELED_DIR, \"*_labeled.npz\")))\n",
        "final_index_rows = []\n",
        "for lp in labeled_npzs:\n",
        "    arr = np.load(lp, allow_pickle=True)\n",
        "    y = arr.get('y', np.array([]))\n",
        "    x = arr.get('x', np.array([]))\n",
        "    # try find original npz by name prefix\n",
        "    baseprefix = os.path.basename(lp).replace(\"_labeled.npz\",\"\")\n",
        "    candidates = sorted(glob.glob(os.path.join(NPZ_FOLDER, f\"*{baseprefix}*.npz\")))\n",
        "    orig = candidates[0] if candidates else \"\"\n",
        "    n_nodes = int(np.asarray(x).shape[0]) if x is not None and np.asarray(x).size else 0\n",
        "    feat_dim = int(np.asarray(x).shape[1]) if (x is not None and np.asarray(x).ndim>1) else 1\n",
        "    yv = float(np.asarray(y).reshape(-1)[0]) if np.asarray(y).size else None\n",
        "    final_index_rows.append({\"npz_file\": orig, \"labeled_npz\": lp, \"n_nodes\": n_nodes, \"feat_dim\": feat_dim, \"y\": yv})\n",
        "\n",
        "all_npz_set = set(sorted(glob.glob(os.path.join(NPZ_FOLDER, \"*.npz\"))))\n",
        "labeled_orig_set = set(r[\"npz_file\"] for r in final_index_rows if r[\"npz_file\"])\n",
        "remaining_npzs = sorted(list(all_npz_set - labeled_orig_set))\n",
        "final_misses_rows = [{\"npz_file\":p, \"npz_sid\": \"\"} for p in remaining_npzs]\n",
        "\n",
        "pd.DataFrame(final_index_rows).to_csv(FINAL_INDEX, index=False)\n",
        "pd.DataFrame(final_misses_rows).to_csv(FINAL_MISSES, index=False)\n",
        "print(\"Wrote final index:\", FINAL_INDEX)\n",
        "print(\"Wrote final misses:\", FINAL_MISSES, \"count:\", len(final_misses_rows))\n",
        "\n",
        "# Build final .pt from labeled npzs\n",
        "try:\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "    data_list = []\n",
        "    for rec in final_index_rows:\n",
        "        lp = rec[\"labeled_npz\"]\n",
        "        if not lp or not os.path.exists(lp): continue\n",
        "        arr = np.load(lp, allow_pickle=True)\n",
        "        x = arr.get('x', np.array([])); pos = arr.get('pos', np.array([])); ei = arr.get('edge_index', np.empty((2,0),dtype=np.int64)); y = arr.get('y', np.array([]))\n",
        "        try:\n",
        "            xt = torch.tensor(np.asarray(x), dtype=torch.float) if np.asarray(x).size else None\n",
        "        except Exception:\n",
        "            xt = None\n",
        "        try: post = torch.tensor(np.asarray(pos), dtype=torch.float) if np.asarray(pos).size else None\n",
        "        except: post = None\n",
        "        try: eit = torch.tensor(np.asarray(ei), dtype=torch.long) if np.asarray(ei).size else None\n",
        "        except: eit = None\n",
        "        try: yt = torch.tensor(np.asarray(y).reshape(-1), dtype=torch.float) if np.asarray(y).size else None\n",
        "        except: yt = None\n",
        "        try:\n",
        "            g = Data(x=xt, edge_index=eit, pos=post, y=yt)\n",
        "            g.subject_id = os.path.basename(lp).replace(\"_labeled.npz\",\"\")\n",
        "            data_list.append(g)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if data_list:\n",
        "        torch.save(data_list, FINAL_PT)\n",
        "        print(\"Saved final .pt with\", len(data_list), \"graphs to:\", FINAL_PT)\n",
        "    else:\n",
        "        print(\"No Data objects created; .pt not saved.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed building .pt:\", e)\n",
        "\n",
        "print(\"Done. Summary:\")\n",
        "print(\"  Newly applied:\", len(applied))\n",
        "print(\"  Total labeled npzs now:\", len(labeled_npzs))\n",
        "print(\"  Remaining misses:\", len(final_misses_rows))\n",
        "print(\"If remaining misses > 0, open the file and either (1) edit and create manual_mappings_to_apply.csv or (2) manually pick keys from the fuzzy suggestions CSV.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-vOl3Eiy8HK",
        "outputId": "76804749-19ab-42f4-bf8b-dfb02d5191c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading fuzzy suggestions: /content/drive/MyDrive/oasis_project/data/graphs/label_misses_fuzzy_suggestions_rf.csv\n",
            "Auto-applied matches above threshold: 46\n",
            "Skipped (below threshold or failed): 0\n",
            "Wrote final index: /content/drive/MyDrive/oasis_project/data/graphs/final_graph_label_index_auto.csv\n",
            "Wrote final misses: /content/drive/MyDrive/oasis_project/data/graphs/final_label_misses_auto.csv count: 0\n",
            "Saved final .pt with 209 graphs to: /content/drive/MyDrive/oasis_project/data/graphs/oasis2_graphs_labeled_auto_full.pt\n",
            "Done. Summary:\n",
            "  Newly applied: 46\n",
            "  Total labeled npzs now: 209\n",
            "  Remaining misses: 0\n",
            "If remaining misses > 0, open the file and either (1) edit and create manual_mappings_to_apply.csv or (2) manually pick keys from the fuzzy suggestions CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_RGLU6yzluV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}